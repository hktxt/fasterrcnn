{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hole fasterRCNN network using vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16\n",
    "from config import opt\n",
    "def decom_vgg16():\n",
    "    # the 30th layer of features is relu of conv5_3\n",
    "    if opt.caffe_pretrain:\n",
    "        model = vgg16(pretrained=False)\n",
    "        if not opt.load_path:\n",
    "            model.load_state_dict(t.load(opt.caffe_pretrain_path))\n",
    "    else:\n",
    "        model = vgg16(not opt.load_path)\n",
    "\n",
    "    features = list(model.features)[:30]\n",
    "    classifier = model.classifier\n",
    "\n",
    "    classifier = list(classifier)\n",
    "    del classifier[6]\n",
    "    if not opt.use_drop:\n",
    "        del classifier[5]\n",
    "        del classifier[2]\n",
    "    classifier = nn.Sequential(*classifier)\n",
    "\n",
    "    # freeze top4 conv\n",
    "    for layer in features[:10]:\n",
    "        for p in layer.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    feat_stride = 16  # downsample 16x for output of conv5 in vgg16\n",
    "    return nn.Sequential(*features), classifier, feat_stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 600, 800])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip = torch.Tensor(1,3,600,800);ip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor, classifier = decom_vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 37, 50])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = extractor(ip);out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -37.254833,  -82.50967 ,   53.254833,   98.50967 ],\n",
       "       [ -82.50967 , -173.01933 ,   98.50967 ,  189.01933 ],\n",
       "       [-173.01933 , -354.03867 ,  189.01933 ,  370.03867 ],\n",
       "       [ -56.      ,  -56.      ,   72.      ,   72.      ],\n",
       "       [-120.      , -120.      ,  136.      ,  136.      ],\n",
       "       [-248.      , -248.      ,  264.      ,  264.      ],\n",
       "       [ -82.50967 ,  -37.254833,   98.50967 ,   53.254833],\n",
       "       [-173.01933 ,  -82.50967 ,  189.01933 ,   98.50967 ],\n",
       "       [-354.03867 , -173.01933 ,  370.03867 ,  189.01933 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import six\n",
    "def generate_anchor_base(base_size=16, ratios=[0.5, 1, 2],\n",
    "                         anchor_scales=[8, 16, 32]):\n",
    "    py = base_size / 2.\n",
    "    px = base_size / 2.\n",
    "\n",
    "    anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4),\n",
    "                           dtype=np.float32)\n",
    "    for i in six.moves.range(len(ratios)):\n",
    "        for j in six.moves.range(len(anchor_scales)):\n",
    "            h = base_size * anchor_scales[j] * np.sqrt(ratios[i])\n",
    "            w = base_size * anchor_scales[j] * np.sqrt(1. / ratios[i])\n",
    "\n",
    "            index = i * len(anchor_scales) + j\n",
    "            anchor_base[index, 0] = py - h / 2.\n",
    "            anchor_base[index, 1] = px - w / 2.\n",
    "            anchor_base[index, 2] = py + h / 2.\n",
    "            anchor_base[index, 3] = px + w / 2.\n",
    "    return anchor_base\n",
    "anchor_base = generate_anchor_base();anchor_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22500, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def enumerate_shifted_anchor(anchor_base, feat_stride=16, height=50, width=50):\n",
    "    # Enumerate all shifted anchors:\n",
    "    #\n",
    "    # add A anchors (1, A, 4) to\n",
    "    # cell K shifts (K, 1, 4) to get\n",
    "    # shift anchors (K, A, 4)\n",
    "    # reshape to (K*A, 4) shifted anchors\n",
    "    # return (K*A, 4)\n",
    "\n",
    "    # !TODO: add support for torch.CudaTensor\n",
    "    # xp = cuda.get_array_module(anchor_base)\n",
    "    # it seems that it can't be boosed using GPU\n",
    "    import numpy as xp\n",
    "    shift_y = xp.arange(0, height * feat_stride, feat_stride)\n",
    "    shift_x = xp.arange(0, width * feat_stride, feat_stride)\n",
    "    shift_x, shift_y = xp.meshgrid(shift_x, shift_y)\n",
    "    shift = xp.stack((shift_y.ravel(), shift_x.ravel(),\n",
    "                      shift_y.ravel(), shift_x.ravel()), axis=1)\n",
    "\n",
    "    A = anchor_base.shape[0]\n",
    "    K = shift.shape[0]\n",
    "    anchor = anchor_base.reshape((1, A, 4)) + \\\n",
    "             shift.reshape((1, K, 4)).transpose((1, 0, 2))\n",
    "    anchor = anchor.reshape((K * A, 4)).astype(np.float32)\n",
    "    return anchor\n",
    "anchor = enumerate_shifted_anchor(anchor_base);anchor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def loc2bbox(src_bbox, loc):\n",
    "    if src_bbox.shape[0] == 0:\n",
    "        return xp.zeros((0, 4), dtype=loc.dtype)\n",
    "\n",
    "    src_bbox = src_bbox.astype(src_bbox.dtype, copy=False)\n",
    "\n",
    "    src_height = src_bbox[:, 2] - src_bbox[:, 0]\n",
    "    src_width = src_bbox[:, 3] - src_bbox[:, 1]\n",
    "    src_ctr_y = src_bbox[:, 0] + 0.5 * src_height\n",
    "    src_ctr_x = src_bbox[:, 1] + 0.5 * src_width\n",
    "\n",
    "    dy = loc[:, 0::4]\n",
    "    dx = loc[:, 1::4]\n",
    "    dh = loc[:, 2::4]\n",
    "    dw = loc[:, 3::4]\n",
    "\n",
    "    ctr_y = dy * src_height[:, np.newaxis] + src_ctr_y[:, np.newaxis]\n",
    "    ctr_x = dx * src_width[:, np.newaxis] + src_ctr_x[:, np.newaxis]\n",
    "    h = np.exp(dh) * src_height[:, np.newaxis]\n",
    "    w = np.exp(dw) * src_width[:, np.newaxis]\n",
    "\n",
    "    dst_bbox = np.zeros(loc.shape, dtype=loc.dtype)\n",
    "    dst_bbox[:, 0::4] = ctr_y - 0.5 * h\n",
    "    dst_bbox[:, 1::4] = ctr_x - 0.5 * w\n",
    "    dst_bbox[:, 2::4] = ctr_y + 0.5 * h\n",
    "    dst_bbox[:, 3::4] = ctr_x + 0.5 * w\n",
    "\n",
    "    return dst_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _non_maximum_suppression_cpu(bbox, thresh, score=None, limit=None):\n",
    "    if len(bbox) == 0:\n",
    "        return np.zeros((0,), dtype=np.int32)\n",
    "\n",
    "    if score is not None:\n",
    "        order = score.argsort()[::-1]\n",
    "        bbox = bbox[order]\n",
    "    bbox_area = np.prod(bbox[:, 2:] - bbox[:, :2], axis=1)\n",
    "\n",
    "    selec = np.zeros(bbox.shape[0], dtype=bool)\n",
    "    for i, b in enumerate(bbox):\n",
    "        tl = np.maximum(b[:2], bbox[selec, :2])\n",
    "        br = np.minimum(b[2:], bbox[selec, 2:])\n",
    "        area = np.prod(br - tl, axis=1) * (tl < br).all(axis=1)\n",
    "\n",
    "        iou = area / (bbox_area[i] + bbox_area[selec] - area)\n",
    "        if (iou >= thresh).any():\n",
    "            continue\n",
    "\n",
    "        selec[i] = True\n",
    "        if limit is not None and np.count_nonzero(selec) >= limit:\n",
    "            break\n",
    "\n",
    "    selec = np.where(selec)[0]\n",
    "    if score is not None:\n",
    "        selec = order[selec]\n",
    "    return selec.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProposalCreator:\n",
    "    def __init__(self,\n",
    "                 parent_model,\n",
    "                 nms_thresh=0.7,\n",
    "                 n_train_pre_nms=12000,\n",
    "                 n_train_post_nms=2000,\n",
    "                 n_test_pre_nms=6000,\n",
    "                 n_test_post_nms=300,\n",
    "                 min_size=16\n",
    "                 ):\n",
    "        self.parent_model = parent_model\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.n_train_pre_nms = n_train_pre_nms\n",
    "        self.n_train_post_nms = n_train_post_nms\n",
    "        self.n_test_pre_nms = n_test_pre_nms\n",
    "        self.n_test_post_nms = n_test_post_nms\n",
    "        self.min_size = min_size\n",
    "\n",
    "    def __call__(self, loc, score,\n",
    "                 anchor, img_size, scale=1.):\n",
    "        # NOTE: when test, remember\n",
    "        # faster_rcnn.eval()\n",
    "        # to set self.traing = False\n",
    "        if self.parent_model.training:\n",
    "            n_pre_nms = self.n_train_pre_nms\n",
    "            n_post_nms = self.n_train_post_nms\n",
    "        else:\n",
    "            n_pre_nms = self.n_test_pre_nms\n",
    "            n_post_nms = self.n_test_post_nms\n",
    "\n",
    "        # Convert anchors into proposal via bbox transformations.\n",
    "        # roi = loc2bbox(anchor, loc)\n",
    "        roi = loc2bbox(anchor, loc)\n",
    "\n",
    "        # Clip predicted boxes to image.\n",
    "        roi[:, slice(0, 4, 2)] = np.clip(\n",
    "            roi[:, slice(0, 4, 2)], 0, img_size[0])\n",
    "        roi[:, slice(1, 4, 2)] = np.clip(\n",
    "            roi[:, slice(1, 4, 2)], 0, img_size[1])\n",
    "\n",
    "        # Remove predicted boxes with either height or width < threshold.\n",
    "        min_size = self.min_size * scale\n",
    "        hs = roi[:, 2] - roi[:, 0]\n",
    "        ws = roi[:, 3] - roi[:, 1]\n",
    "        keep = np.where((hs >= min_size) & (ws >= min_size))[0]\n",
    "        roi = roi[keep, :]\n",
    "        score = score[keep]\n",
    "\n",
    "        # Sort all (proposal, score) pairs by score from highest to lowest.\n",
    "        # Take top pre_nms_topN (e.g. 6000).\n",
    "        order = score.ravel().argsort()[::-1]\n",
    "        if n_pre_nms > 0:\n",
    "            order = order[:n_pre_nms]\n",
    "        roi = roi[order, :]\n",
    "\n",
    "        # Apply nms (e.g. threshold = 0.7).\n",
    "        # Take after_nms_topN (e.g. 300).\n",
    "\n",
    "        # unNOTE: somthing is wrong here!\n",
    "        # TODO: remove cuda.to_gpu\n",
    "        keep = _non_maximum_suppression_cpu(\n",
    "            np.ascontiguousarray(np.asarray(roi)),\n",
    "            thresh=self.nms_thresh)\n",
    "        if n_post_nms > 0:\n",
    "            keep = keep[:n_post_nms]\n",
    "        roi = roi[keep]\n",
    "        return roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "class RegionProposalNetwork(nn.Module):\n",
    "    def __init__(\n",
    "                self, in_channels=512, mid_channels=512, ratios=[0.5, 1, 2],\n",
    "                anchor_scales=[8, 16, 32], feat_stride=16,\n",
    "                proposal_creator_params=dict()):\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "        \n",
    "        self.anchor_base = generate_anchor_base(anchor_scales=anchor_scales, ratios=ratios)\n",
    "        self.feat_stride = feat_stride\n",
    "        self.proposal_layer = ProposalCreator(self, **proposal_creator_params)\n",
    "        n_anchor = self.anchor_base.shape[0]\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1)\n",
    "        self.score = nn.Conv2d(mid_channels, n_anchor * 2, 1, 1, 0)\n",
    "        self.loc = nn.Conv2d(mid_channels, n_anchor * 4, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x, img_size, scale=1.):\n",
    "        n, _, hh, ww = x.shape\n",
    "        anchor = enumerate_shifted_anchor(np.array(self.anchor_base), self.feat_stride, hh, ww) # anchor:(22500, 4)\n",
    "\n",
    "        n_anchor = anchor.shape[0] // (hh * ww)\n",
    "        \n",
    "        h = F.relu(self.conv1(x)) # torch.Size([1, 512, 50, 50]) for base feature 50*50 from 800*800\n",
    "        rpn_locs = self.loc(h) # torch.Size([1, 36, 50, 50])\n",
    "        rpn_locs = rpn_locs.permute(0, 2, 3, 1).contiguous().view(n, -1, 4) # torch.Size([1, 22500, 4])\n",
    "        rpn_scores = self.score(h) # torch.Size([1, 18, 50, 50])\n",
    "        rpn_scores = rpn_scores.permute(0, 2, 3, 1).contiguous() # torch.Size([1, 50, 50, 18])\n",
    "        rpn_softmax_scores = F.softmax(rpn_scores.view(n, hh, ww, n_anchor, 2), dim=4) # torch.Size([1, 50, 50, 9, 2])\n",
    "        rpn_fg_scores = rpn_softmax_scores[:, :, :, :, 1].contiguous() # torch.Size([1, 50, 50, 9])\n",
    "        rpn_fg_scores = rpn_fg_scores.view(n, -1) # torch.Size([1, 22500])\n",
    "        rpn_scores = rpn_scores.view(n, -1, 2) # torch.Size([1, 22500, 2])\n",
    "\n",
    "        rois = list()\n",
    "        roi_indices = list()\n",
    "        for i in range(n):\n",
    "            # ~(2000, 4)\n",
    "            roi = self.proposal_layer(rpn_locs[i].cpu().data.numpy(),        # (22500, 4)\n",
    "                                        rpn_fg_scores[i].cpu().data.numpy(), # (22500,)\n",
    "                                        anchor, img_size,\n",
    "                                        scale=scale)\n",
    "            batch_index = i * np.ones((len(roi),), dtype=np.int32)\n",
    "            rois.append(roi)\n",
    "            roi_indices.append(batch_index)\n",
    "\n",
    "        rois = np.concatenate(rois, axis=0)\n",
    "        roi_indices = np.concatenate(roi_indices, axis=0)\n",
    "        \n",
    "        return rpn_locs, rpn_scores, rois, roi_indices, anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegionProposalNetwork(\n",
       "  (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (score): Conv2d(512, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (loc): Conv2d(512, 36, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tnet = RegionProposalNetwork(512,512,ratios=[0.5, 1, 2], anchor_scales=[8, 16, 32],feat_stride=16);tnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_locs, rpn_scores, rois, roi_indices, anchor = tnet(torch.Tensor(2,512,50,50), (800,800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RoIPooling2D():\n",
    "\n",
    "    return adaptive_max_pool2d((7,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16RoIHead(nn.Module):\n",
    "    \"\"\"Faster R-CNN Head for VGG-16 based implementation.\n",
    "    This class is used as a head for Faster R-CNN.\n",
    "    This outputs class-wise localizations and classification based on feature\n",
    "    maps in the given RoIs.\n",
    "    \n",
    "    Args:\n",
    "        n_class (int): The number of classes possibly including the background.\n",
    "        roi_size (int): Height and width of the feature maps after RoI-pooling.\n",
    "        spatial_scale (float): Scale of the roi is resized.\n",
    "        classifier (nn.Module): Two layer Linear ported from vgg16\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_class, roi_size, spatial_scale,\n",
    "                 classifier):\n",
    "        # n_class includes the background\n",
    "        super(VGG16RoIHead, self).__init__()\n",
    "\n",
    "        self.classifier = classifier\n",
    "        self.cls_loc = nn.Linear(4096, n_class * 4)\n",
    "        self.score = nn.Linear(4096, n_class)\n",
    "\n",
    "        self.n_class = n_class\n",
    "        self.roi_size = roi_size\n",
    "        self.spatial_scale = spatial_scale\n",
    "        self.roi = RoIPooling2D(self.roi_size, self.roi_size, self.spatial_scale)\n",
    "\n",
    "    def forward(self, x, rois, roi_indices):\n",
    "        \"\"\"Forward the chain.\n",
    "        We assume that there are :math:`N` batches.\n",
    "        Args:\n",
    "            x (Variable): 4D image variable.\n",
    "            rois (Tensor): A bounding box array containing coordinates of\n",
    "                proposal boxes.  This is a concatenation of bounding box\n",
    "                arrays from multiple images in the batch.\n",
    "                Its shape is :math:`(R', 4)`. Given :math:`R_i` proposed\n",
    "                RoIs from the :math:`i` th image,\n",
    "                :math:`R' = \\\\sum _{i=1} ^ N R_i`.\n",
    "            roi_indices (Tensor): An array containing indices of images to\n",
    "                which bounding boxes correspond to. Its shape is :math:`(R',)`.\n",
    "        \"\"\"\n",
    "        # in case roi_indices is  ndarray\n",
    "        roi_indices = at.totensor(roi_indices).float()\n",
    "        rois = at.totensor(rois).float()\n",
    "        indices_and_rois = t.cat([roi_indices[:, None], rois], dim=1)\n",
    "        # NOTE: important: yx->xy\n",
    "        xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]]\n",
    "        indices_and_rois =  xy_indices_and_rois.contiguous()\n",
    "\n",
    "        pool = self.roi(x, indices_and_rois)\n",
    "        pool = pool.view(pool.size(0), -1)\n",
    "        fc7 = self.classifier(pool)\n",
    "        roi_cls_locs = self.cls_loc(fc7)\n",
    "        roi_scores = self.score(fc7)\n",
    "        return roi_cls_locs, roi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fasterRCNN(nn.Module):\n",
    "    def __init__(self, n_fg_class=20, ratios=[0.5, 1, 2], anchor_scales=[8, 16, 32]):\n",
    "        super().__init__()\n",
    "        self.extractor, self.classifier, self.feat_stride = decom_vgg16()\n",
    "        \n",
    "        self.rpn = RegionProposalNetwork(512, 512,\n",
    "                                        ratios=ratios,\n",
    "                                        anchor_scales=anchor_scales,\n",
    "                                        feat_stride=16)\n",
    "        self.head = VGG16RoIHead(\n",
    "                                n_class=n_fg_class + 1,\n",
    "                                roi_size=7,\n",
    "                                spatial_scale=(1. / self.feat_stride),\n",
    "                                classifier=self.classifier)\n",
    "        \n",
    "    def forward(self, x, scale):\n",
    "        _, _, H, W = x.shape\n",
    "        img_size = (H, W)\n",
    "        x = self.extractor(x)\n",
    "        rpn_locs, rpn_scores, rois, roi_indices, anchor = self.rpn(x, img_size, scale)\n",
    "        roi_cls_locs, roi_scores = self.head(x, rois, roi_indices)\n",
    "\n",
    "        return rpn_locs, rpn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = fasterRCNN()\n",
    "out = net(ip,1.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0720, -0.0099, -0.0260,  0.0288],\n",
       "          [-0.0436, -0.0548, -0.0776,  0.0275],\n",
       "          [ 0.0278, -0.0525,  0.0029, -0.0219],\n",
       "          ...,\n",
       "          [ 0.0015, -0.0238,  0.0241, -0.0242],\n",
       "          [-0.0427, -0.0378,  0.0023,  0.0358],\n",
       "          [ 0.0336, -0.0212,  0.0493, -0.0318]]], grad_fn=<ViewBackward>),\n",
       " tensor([[[ 0.0460,  0.0253],\n",
       "          [ 0.0161,  0.0203],\n",
       "          [-0.0431,  0.0404],\n",
       "          ...,\n",
       "          [-0.0250,  0.0290],\n",
       "          [ 0.0219, -0.0705],\n",
       "          [-0.0255,  0.0011]]], grad_fn=<ViewBackward>),\n",
       " array([[  0.      ,   0.      , 162.82504 , 153.29471 ],\n",
       "        [  0.      ,   0.      , 193.1619  , 340.03793 ],\n",
       "        [  0.      , 300.58716 , 204.72395 , 800.      ],\n",
       "        ...,\n",
       "        [126.0621  , 386.3034  , 299.4294  , 762.36835 ],\n",
       "        [110.0621  ,  82.303406, 283.4294  , 458.36835 ],\n",
       "        [ 46.062103, 274.3034  , 219.42941 , 650.36835 ]], dtype=float32),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([[ -37.254833,  -82.50967 ,   53.254833,   98.50967 ],\n",
       "        [ -82.50967 , -173.01933 ,   98.50967 ,  189.01933 ],\n",
       "        [-173.01933 , -354.03867 ,  189.01933 ,  370.03867 ],\n",
       "        ...,\n",
       "        [ 493.49033 ,  746.7452  ,  674.50964 ,  837.2548  ],\n",
       "        [ 402.98065 ,  701.49036 ,  765.01935 ,  882.50964 ],\n",
       "        [ 221.96133 ,  610.98065 ,  946.0387  ,  973.01935 ]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
