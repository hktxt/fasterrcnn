{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hole fasterRCNN network using vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16\n",
    "from config import opt\n",
    "def decom_vgg16():\n",
    "    # the 30th layer of features is relu of conv5_3\n",
    "    if opt.caffe_pretrain:\n",
    "        model = vgg16(pretrained=False)\n",
    "        if not opt.load_path:\n",
    "            model.load_state_dict(t.load(opt.caffe_pretrain_path))\n",
    "    else:\n",
    "        model = vgg16(not opt.load_path)\n",
    "\n",
    "    features = list(model.features)[:30]\n",
    "    classifier = model.classifier\n",
    "\n",
    "    classifier = list(classifier)\n",
    "    del classifier[6]\n",
    "    if not opt.use_drop:\n",
    "        del classifier[5]\n",
    "        del classifier[2]\n",
    "    classifier = nn.Sequential(*classifier)\n",
    "\n",
    "    # freeze top4 conv\n",
    "    for layer in features[:10]:\n",
    "        for p in layer.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    return nn.Sequential(*features), classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 600, 800])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip = torch.Tensor(1,3,600,800);ip.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor, classifier = decom_vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 37, 50])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = extractor(ip);out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import six\n",
    "def generate_anchor_base(base_size=16, ratios=[0.5, 1, 2],\n",
    "                         anchor_scales=[8, 16, 32]):\n",
    "    py = base_size / 2.\n",
    "    px = base_size / 2.\n",
    "\n",
    "    anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4),\n",
    "                           dtype=np.float32)\n",
    "    for i in six.moves.range(len(ratios)):\n",
    "        for j in six.moves.range(len(anchor_scales)):\n",
    "            h = base_size * anchor_scales[j] * np.sqrt(ratios[i])\n",
    "            w = base_size * anchor_scales[j] * np.sqrt(1. / ratios[i])\n",
    "\n",
    "            index = i * len(anchor_scales) + j\n",
    "            anchor_base[index, 0] = py - h / 2.\n",
    "            anchor_base[index, 1] = px - w / 2.\n",
    "            anchor_base[index, 2] = py + h / 2.\n",
    "            anchor_base[index, 3] = px + w / 2.\n",
    "    return anchor_base\n",
    "#anchor_base = generate_anchor_base();anchor_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22500, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def enumerate_shifted_anchor(anchor_base, feat_stride=16, height=50, width=50):\n",
    "    # Enumerate all shifted anchors:\n",
    "    #\n",
    "    # add A anchors (1, A, 4) to\n",
    "    # cell K shifts (K, 1, 4) to get\n",
    "    # shift anchors (K, A, 4)\n",
    "    # reshape to (K*A, 4) shifted anchors\n",
    "    # return (K*A, 4)\n",
    "\n",
    "    # !TODO: add support for torch.CudaTensor\n",
    "    # xp = cuda.get_array_module(anchor_base)\n",
    "    # it seems that it can't be boosed using GPU\n",
    "    import numpy as xp\n",
    "    shift_y = xp.arange(0, height * feat_stride, feat_stride)\n",
    "    shift_x = xp.arange(0, width * feat_stride, feat_stride)\n",
    "    shift_x, shift_y = xp.meshgrid(shift_x, shift_y)\n",
    "    shift = xp.stack((shift_y.ravel(), shift_x.ravel(),\n",
    "                      shift_y.ravel(), shift_x.ravel()), axis=1)\n",
    "\n",
    "    A = anchor_base.shape[0]\n",
    "    K = shift.shape[0]\n",
    "    anchor = anchor_base.reshape((1, A, 4)) + \\\n",
    "             shift.reshape((1, K, 4)).transpose((1, 0, 2))\n",
    "    anchor = anchor.reshape((K * A, 4)).astype(np.float32)\n",
    "    return anchor\n",
    "anchor = enumerate_shifted_anchor(anchor_base);anchor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def loc2bbox(src_bbox, loc):\n",
    "    if src_bbox.shape[0] == 0:\n",
    "        return xp.zeros((0, 4), dtype=loc.dtype)\n",
    "\n",
    "    src_bbox = src_bbox.astype(src_bbox.dtype, copy=False)\n",
    "\n",
    "    src_height = src_bbox[:, 2] - src_bbox[:, 0]\n",
    "    src_width = src_bbox[:, 3] - src_bbox[:, 1]\n",
    "    src_ctr_y = src_bbox[:, 0] + 0.5 * src_height\n",
    "    src_ctr_x = src_bbox[:, 1] + 0.5 * src_width\n",
    "\n",
    "    dy = loc[:, 0::4]\n",
    "    dx = loc[:, 1::4]\n",
    "    dh = loc[:, 2::4]\n",
    "    dw = loc[:, 3::4]\n",
    "\n",
    "    ctr_y = dy * src_height[:, np.newaxis] + src_ctr_y[:, np.newaxis]\n",
    "    ctr_x = dx * src_width[:, np.newaxis] + src_ctr_x[:, np.newaxis]\n",
    "    h = np.exp(dh) * src_height[:, np.newaxis]\n",
    "    w = np.exp(dw) * src_width[:, np.newaxis]\n",
    "\n",
    "    dst_bbox = np.zeros(loc.shape, dtype=loc.dtype)\n",
    "    dst_bbox[:, 0::4] = ctr_y - 0.5 * h\n",
    "    dst_bbox[:, 1::4] = ctr_x - 0.5 * w\n",
    "    dst_bbox[:, 2::4] = ctr_y + 0.5 * h\n",
    "    dst_bbox[:, 3::4] = ctr_x + 0.5 * w\n",
    "\n",
    "    return dst_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _non_maximum_suppression_cpu(bbox, thresh, score=None, limit=None):\n",
    "    if len(bbox) == 0:\n",
    "        return np.zeros((0,), dtype=np.int32)\n",
    "\n",
    "    if score is not None:\n",
    "        order = score.argsort()[::-1]\n",
    "        bbox = bbox[order]\n",
    "    bbox_area = np.prod(bbox[:, 2:] - bbox[:, :2], axis=1)\n",
    "\n",
    "    selec = np.zeros(bbox.shape[0], dtype=bool)\n",
    "    for i, b in enumerate(bbox):\n",
    "        tl = np.maximum(b[:2], bbox[selec, :2])\n",
    "        br = np.minimum(b[2:], bbox[selec, 2:])\n",
    "        area = np.prod(br - tl, axis=1) * (tl < br).all(axis=1)\n",
    "\n",
    "        iou = area / (bbox_area[i] + bbox_area[selec] - area)\n",
    "        if (iou >= thresh).any():\n",
    "            continue\n",
    "\n",
    "        selec[i] = True\n",
    "        if limit is not None and np.count_nonzero(selec) >= limit:\n",
    "            break\n",
    "\n",
    "    selec = np.where(selec)[0]\n",
    "    if score is not None:\n",
    "        selec = order[selec]\n",
    "    return selec.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProposalCreator:\n",
    "    def __init__(self,\n",
    "                 parent_model,\n",
    "                 nms_thresh=0.7,\n",
    "                 n_train_pre_nms=12000,\n",
    "                 n_train_post_nms=2000,\n",
    "                 n_test_pre_nms=6000,\n",
    "                 n_test_post_nms=300,\n",
    "                 min_size=16\n",
    "                 ):\n",
    "        self.parent_model = parent_model\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.n_train_pre_nms = n_train_pre_nms\n",
    "        self.n_train_post_nms = n_train_post_nms\n",
    "        self.n_test_pre_nms = n_test_pre_nms\n",
    "        self.n_test_post_nms = n_test_post_nms\n",
    "        self.min_size = min_size\n",
    "\n",
    "    def __call__(self, loc, score,\n",
    "                 anchor, img_size, scale=1.):\n",
    "        # NOTE: when test, remember\n",
    "        # faster_rcnn.eval()\n",
    "        # to set self.traing = False\n",
    "        if self.parent_model.training:\n",
    "            n_pre_nms = self.n_train_pre_nms\n",
    "            n_post_nms = self.n_train_post_nms\n",
    "        else:\n",
    "            n_pre_nms = self.n_test_pre_nms\n",
    "            n_post_nms = self.n_test_post_nms\n",
    "\n",
    "        # Convert anchors into proposal via bbox transformations.\n",
    "        # roi = loc2bbox(anchor, loc)\n",
    "        roi = loc2bbox(anchor, loc)\n",
    "\n",
    "        # Clip predicted boxes to image.\n",
    "        roi[:, slice(0, 4, 2)] = np.clip(\n",
    "            roi[:, slice(0, 4, 2)], 0, img_size[0])\n",
    "        roi[:, slice(1, 4, 2)] = np.clip(\n",
    "            roi[:, slice(1, 4, 2)], 0, img_size[1])\n",
    "\n",
    "        # Remove predicted boxes with either height or width < threshold.\n",
    "        min_size = self.min_size * scale\n",
    "        hs = roi[:, 2] - roi[:, 0]\n",
    "        ws = roi[:, 3] - roi[:, 1]\n",
    "        keep = np.where((hs >= min_size) & (ws >= min_size))[0]\n",
    "        roi = roi[keep, :]\n",
    "        score = score[keep]\n",
    "\n",
    "        # Sort all (proposal, score) pairs by score from highest to lowest.\n",
    "        # Take top pre_nms_topN (e.g. 6000).\n",
    "        order = score.ravel().argsort()[::-1]\n",
    "        if n_pre_nms > 0:\n",
    "            order = order[:n_pre_nms]\n",
    "        roi = roi[order, :]\n",
    "\n",
    "        # Apply nms (e.g. threshold = 0.7).\n",
    "        # Take after_nms_topN (e.g. 300).\n",
    "\n",
    "        # unNOTE: somthing is wrong here!\n",
    "        # TODO: remove cuda.to_gpu\n",
    "        keep = _non_maximum_suppression_cpu(\n",
    "            np.ascontiguousarray(np.asarray(roi)),\n",
    "            thresh=self.nms_thresh)\n",
    "        if n_post_nms > 0:\n",
    "            keep = keep[:n_post_nms]\n",
    "        roi = roi[keep]\n",
    "        return roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "class RegionProposalNetwork(nn.Module):\n",
    "    def __init__(\n",
    "                self, in_channels=512, mid_channels=512, ratios=[0.5, 1, 2],\n",
    "                anchor_scales=[8, 16, 32], feat_stride=16,\n",
    "                proposal_creator_params=dict()):\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "        \n",
    "        self.anchor_base = generate_anchor_base(anchor_scales=anchor_scales, ratios=ratios)\n",
    "        self.feat_stride = feat_stride\n",
    "        self.proposal_layer = ProposalCreator(self, **proposal_creator_params)\n",
    "        n_anchor = self.anchor_base.shape[0]\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1)\n",
    "        self.score = nn.Conv2d(mid_channels, n_anchor * 2, 1, 1, 0)\n",
    "        self.loc = nn.Conv2d(mid_channels, n_anchor * 4, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x, img_size, scale=1.):\n",
    "        n, _, hh, ww = x.shape\n",
    "        anchor = enumerate_shifted_anchor(np.array(self.anchor_base), self.feat_stride, hh, ww) # anchor:(22500, 4)\n",
    "\n",
    "        n_anchor = anchor.shape[0] // (hh * ww)\n",
    "        \n",
    "        h = F.relu(self.conv1(x)) # torch.Size([1, 512, 50, 50]) for base feature 50*50 from 800*800\n",
    "        rpn_locs = self.loc(h) # torch.Size([1, 36, 50, 50])\n",
    "        rpn_locs = rpn_locs.permute(0, 2, 3, 1).contiguous().view(n, -1, 4) # torch.Size([1, 22500, 4])\n",
    "        rpn_scores = self.score(h) # torch.Size([1, 18, 50, 50])\n",
    "        rpn_scores = rpn_scores.permute(0, 2, 3, 1).contiguous() # torch.Size([1, 50, 50, 18])\n",
    "        rpn_softmax_scores = F.softmax(rpn_scores.view(n, hh, ww, n_anchor, 2), dim=4) # torch.Size([1, 50, 50, 9, 2])\n",
    "        rpn_fg_scores = rpn_softmax_scores[:, :, :, :, 1].contiguous() # torch.Size([1, 50, 50, 9])\n",
    "        rpn_fg_scores = rpn_fg_scores.view(n, -1) # torch.Size([1, 22500])\n",
    "        rpn_scores = rpn_scores.view(n, -1, 2) # torch.Size([1, 22500, 2])\n",
    "\n",
    "        rois = list()\n",
    "        roi_indices = list()\n",
    "        for i in range(n):\n",
    "            # ~(2000, 4)\n",
    "            roi = self.proposal_layer(rpn_locs[i].cpu().data.numpy(),        # (22500, 4)\n",
    "                                        rpn_fg_scores[i].cpu().data.numpy(), # (22500,)\n",
    "                                        anchor, img_size,\n",
    "                                        scale=scale)\n",
    "            batch_index = i * np.ones((len(roi),), dtype=np.int32)\n",
    "            rois.append(roi)\n",
    "            roi_indices.append(batch_index)\n",
    "\n",
    "        rois = np.concatenate(rois, axis=0)\n",
    "        roi_indices = np.concatenate(roi_indices, axis=0)\n",
    "        \n",
    "        return rpn_locs, rpn_scores, rois, roi_indices, anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegionProposalNetwork(\n",
       "  (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (score): Conv2d(512, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (loc): Conv2d(512, 36, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tnet = RegionProposalNetwork(512,512,ratios=[0.5, 1, 2], anchor_scales=[8, 16, 32],feat_stride=16);tnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn_locs, rpn_scores, rois, roi_indices, anchor = tnet(torch.Tensor(2,512,50,50), (800,800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoIPooling2D(nn.Module):\n",
    "\n",
    "    def __init__(self, outh, outw, spatial_scale):\n",
    "        super(RoIPooling2D, self).__init__()\n",
    "        self.RoI = RoI(outh, outw, spatial_scale)\n",
    "\n",
    "    def forward(self, x, rois):\n",
    "        return self.RoI(x, rois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.function import Function\n",
    "from torch._thnn import type2backend\n",
    "class AdaptiveMaxPool2d(Function):\n",
    "    def __init__(self, out_w, out_h):\n",
    "        super(AdaptiveMaxPool2d, self).__init__()\n",
    "        self.out_w = out_w\n",
    "        self.out_h = out_h\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input.new()\n",
    "        indices = input.new().long()\n",
    "        self.save_for_backward(input)\n",
    "        self.indices = indices\n",
    "        self._backend = type2backend[type(input)]\n",
    "        self._backend.SpatialAdaptiveMaxPooling_updateOutput(\n",
    "                self._backend.library_state, input, output, indices,\n",
    "                self.out_w, self.out_h)\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        input, = self.saved_tensors\n",
    "        indices = self.indices\n",
    "        grad_input = grad_output.new()\n",
    "        self._backend.SpatialAdaptiveMaxPooling_updateGradInput(\n",
    "                self._backend.library_state, input, grad_output, grad_input,\n",
    "                indices)\n",
    "        return grad_input, None\n",
    "\n",
    "def adaptive_max_pool(input, size):\n",
    "    return AdaptiveMaxPool2d(size[0],size[1])(input)\n",
    "\n",
    "def roi_pooling(input, rois, size=(7,7), spatial_scale=1.0):\n",
    "    assert(rois.dim() == 2)\n",
    "    assert(rois.size(1) == 5)\n",
    "    output = []\n",
    "    rois = rois.data.float()\n",
    "    num_rois = rois.size(0)\n",
    "    \n",
    "    rois[:,1:].mul_(spatial_scale)\n",
    "    rois = rois.long()\n",
    "    for i in range(num_rois):\n",
    "        roi = rois[i]\n",
    "        im_idx = roi[0]\n",
    "        im = input.narrow(0, im_idx, 1)[..., roi[2]:(roi[4]+1), roi[1]:(roi[3]+1)]\n",
    "        output.append(adaptive_max_pool(im, size))\n",
    "\n",
    "    return torch.cat(output, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoI(Function):\n",
    "    def __init__(self, outh, outw, spatial_scale):\n",
    "        self.forward_fn = load_kernel('roi_forward', kernel_forward)\n",
    "        self.backward_fn = load_kernel('roi_backward', kernel_backward)\n",
    "        self.outh, self.outw, self.spatial_scale = outh, outw, spatial_scale\n",
    "\n",
    "    def forward(self, x, rois):\n",
    "        # NOTE: MAKE SURE input is contiguous too\n",
    "        x = x.contiguous()\n",
    "        rois = rois.contiguous()\n",
    "        self.in_size = B, C, H, W = x.size()\n",
    "        self.N = N = rois.size(0)\n",
    "        output = t.zeros(N, C, self.outh, self.outw).cuda()\n",
    "        self.argmax_data = t.zeros(N, C, self.outh, self.outw).int().cuda()\n",
    "        self.rois = rois\n",
    "        args = [x.data_ptr(), rois.data_ptr(),\n",
    "                output.data_ptr(),\n",
    "                self.argmax_data.data_ptr(),\n",
    "                self.spatial_scale, C, H, W,\n",
    "                self.outh, self.outw,\n",
    "                output.numel()]\n",
    "        stream = Stream(ptr=torch.cuda.current_stream().cuda_stream)\n",
    "        self.forward_fn(args=args,\n",
    "                        block=(CUDA_NUM_THREADS, 1, 1),\n",
    "                        grid=(GET_BLOCKS(output.numel()), 1, 1),\n",
    "                        stream=stream)\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        ##NOTE: IMPORTANT CONTIGUOUS\n",
    "        # TODO: input\n",
    "        grad_output = grad_output.contiguous()\n",
    "        B, C, H, W = self.in_size\n",
    "        grad_input = t.zeros(self.in_size).cuda()\n",
    "        stream = Stream(ptr=torch.cuda.current_stream().cuda_stream)\n",
    "        args = [grad_output.data_ptr(),\n",
    "                self.argmax_data.data_ptr(),\n",
    "                self.rois.data_ptr(),\n",
    "                grad_input.data_ptr(),\n",
    "                self.N, self.spatial_scale, C, H, W, self.outh, self.outw,\n",
    "                grad_input.numel()]\n",
    "        self.backward_fn(args=args,\n",
    "                         block=(CUDA_NUM_THREADS, 1, 1),\n",
    "                         grid=(GET_BLOCKS(grad_input.numel()), 1, 1),\n",
    "                         stream=stream\n",
    "                         )\n",
    "        return grad_input, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16RoIHead(nn.Module):\n",
    "    def __init__(self, n_class, roi_size, spatial_scale,\n",
    "                 classifier):\n",
    "        # n_class includes the background\n",
    "        super(VGG16RoIHead, self).__init__()\n",
    "\n",
    "        self.classifier = classifier\n",
    "        self.cls_loc = nn.Linear(4096, n_class * 4)\n",
    "        self.score = nn.Linear(4096, n_class)\n",
    "\n",
    "        self.n_class = n_class\n",
    "        self.roi_size = roi_size\n",
    "        self.spatial_scale = spatial_scale\n",
    "        #self.roi = RoIPooling2D(self.roi_size, self.roi_size, self.spatial_scale)\n",
    "        self.roi = roi_pooling()\n",
    "\n",
    "    def forward(self, x, rois, roi_indices):\n",
    "        # in case roi_indices is  ndarray\n",
    "        roi_indices = at.totensor(roi_indices).float()\n",
    "        rois = at.totensor(rois).float()\n",
    "        indices_and_rois = t.cat([roi_indices[:, None], rois], dim=1)\n",
    "        # NOTE: important: yx->xy\n",
    "        xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]]\n",
    "        indices_and_rois =  xy_indices_and_rois.contiguous()\n",
    "\n",
    "        pool = self.roi(x, indices_and_rois, (self.roi_size, self.roi_size))\n",
    "        pool = pool.view(pool.size(0), -1)\n",
    "        fc7 = self.classifier(pool)\n",
    "        roi_cls_locs = self.cls_loc(fc7)\n",
    "        roi_scores = self.score(fc7)\n",
    "        return roi_cls_locs, roi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fasterrcnn(nn.Module):\n",
    "    def __init__(self, n_fg_class=20, ratios=[0.5, 1, 2], anchor_scales=[8, 16, 32]):\n",
    "        super().__init__()\n",
    "        self.extractor, self.classifier, self.feat_stride = decom_vgg16()\n",
    "        \n",
    "        self.rpn = RegionProposalNetwork(512, 512,\n",
    "                                        ratios=ratios,\n",
    "                                        anchor_scales=anchor_scales,\n",
    "                                        feat_stride=16)\n",
    "        self.head = VGG16RoIHead(\n",
    "                                n_class=n_fg_class + 1,\n",
    "                                roi_size=7,\n",
    "                                spatial_scale=(1. / self.feat_stride),\n",
    "                                classifier=self.classifier)\n",
    "        \n",
    "    def forward(self, x, scale):\n",
    "        _, _, H, W = x.shape\n",
    "        img_size = (H, W)\n",
    "        x = self.extractor(x)\n",
    "        rpn_locs, rpn_scores, rois, roi_indices, anchor = self.rpn(x, img_size, scale)\n",
    "        roi_cls_locs, roi_scores = self.head(x, rois, roi_indices)\n",
    "\n",
    "        return rpn_locs, rpn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = fasterrcnn()\n",
    "out = net(ip,1.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0720, -0.0099, -0.0260,  0.0288],\n",
       "          [-0.0436, -0.0548, -0.0776,  0.0275],\n",
       "          [ 0.0278, -0.0525,  0.0029, -0.0219],\n",
       "          ...,\n",
       "          [ 0.0015, -0.0238,  0.0241, -0.0242],\n",
       "          [-0.0427, -0.0378,  0.0023,  0.0358],\n",
       "          [ 0.0336, -0.0212,  0.0493, -0.0318]]], grad_fn=<ViewBackward>),\n",
       " tensor([[[ 0.0460,  0.0253],\n",
       "          [ 0.0161,  0.0203],\n",
       "          [-0.0431,  0.0404],\n",
       "          ...,\n",
       "          [-0.0250,  0.0290],\n",
       "          [ 0.0219, -0.0705],\n",
       "          [-0.0255,  0.0011]]], grad_fn=<ViewBackward>),\n",
       " array([[  0.      ,   0.      , 162.82504 , 153.29471 ],\n",
       "        [  0.      ,   0.      , 193.1619  , 340.03793 ],\n",
       "        [  0.      , 300.58716 , 204.72395 , 800.      ],\n",
       "        ...,\n",
       "        [126.0621  , 386.3034  , 299.4294  , 762.36835 ],\n",
       "        [110.0621  ,  82.303406, 283.4294  , 458.36835 ],\n",
       "        [ 46.062103, 274.3034  , 219.42941 , 650.36835 ]], dtype=float32),\n",
       " array([0, 0, 0, ..., 0, 0, 0]),\n",
       " array([[ -37.254833,  -82.50967 ,   53.254833,   98.50967 ],\n",
       "        [ -82.50967 , -173.01933 ,   98.50967 ,  189.01933 ],\n",
       "        [-173.01933 , -354.03867 ,  189.01933 ,  370.03867 ],\n",
       "        ...,\n",
       "        [ 493.49033 ,  746.7452  ,  674.50964 ,  837.2548  ],\n",
       "        [ 402.98065 ,  701.49036 ,  765.01935 ,  882.50964 ],\n",
       "        [ 221.96133 ,  610.98065 ,  946.0387  ,  973.01935 ]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nograd(f):\n",
    "    def new_f(*args,**kwargs):\n",
    "        with t.no_grad():\n",
    "            return f(*args,**kwargs)\n",
    "    return new_f\n",
    "\n",
    "class FasterRCNN(nn.Module):\n",
    "    def __init__(self, extractor, rpn, head,\n",
    "                loc_normalize_mean = (0., 0., 0., 0.),\n",
    "                loc_normalize_std = (0.1, 0.1, 0.2, 0.2)\n",
    "    ):\n",
    "        super(FasterRCNN, self).__init__()\n",
    "        self.extractor = extractor\n",
    "        self.rpn = rpn\n",
    "        self.head = head\n",
    "\n",
    "        # mean and std\n",
    "        self.loc_normalize_mean = loc_normalize_mean\n",
    "        self.loc_normalize_std = loc_normalize_std\n",
    "        self.use_preset('evaluate')\n",
    "\n",
    "    @property\n",
    "    def n_class(self):\n",
    "        # Total number of classes including the background.\n",
    "        return self.head.n_class\n",
    "\n",
    "    def forward(self, x, scale=1.):\n",
    "        img_size = x.shape[2:]\n",
    "\n",
    "        h = self.extractor(x)\n",
    "        rpn_locs, rpn_scores, rois, roi_indices, anchor = \\\n",
    "            self.rpn(h, img_size, scale)\n",
    "        roi_cls_locs, roi_scores = self.head(\n",
    "            h, rois, roi_indices)\n",
    "        return roi_cls_locs, roi_scores, rois, roi_indices\n",
    "\n",
    "    def use_preset(self, preset):\n",
    "        if preset == 'visualize':\n",
    "            self.nms_thresh = 0.3\n",
    "            self.score_thresh = 0.7\n",
    "        elif preset == 'evaluate':\n",
    "            self.nms_thresh = 0.3\n",
    "            self.score_thresh = 0.05\n",
    "        else:\n",
    "            raise ValueError('preset must be visualize or evaluate')\n",
    "\n",
    "    def _suppress(self, raw_cls_bbox, raw_prob):\n",
    "        bbox = list()\n",
    "        label = list()\n",
    "        score = list()\n",
    "        # skip cls_id = 0 because it is the background class\n",
    "        for l in range(1, self.n_class):\n",
    "            cls_bbox_l = raw_cls_bbox.reshape((-1, self.n_class, 4))[:, l, :]\n",
    "            prob_l = raw_prob[:, l]\n",
    "            mask = prob_l > self.score_thresh\n",
    "            cls_bbox_l = cls_bbox_l[mask]\n",
    "            prob_l = prob_l[mask]\n",
    "            keep = non_maximum_suppression(\n",
    "                cp.array(cls_bbox_l), self.nms_thresh, prob_l)\n",
    "            keep = cp.asnumpy(keep)\n",
    "            bbox.append(cls_bbox_l[keep])\n",
    "            # The labels are in [0, self.n_class - 2].\n",
    "            label.append((l - 1) * np.ones((len(keep),)))\n",
    "            score.append(prob_l[keep])\n",
    "        bbox = np.concatenate(bbox, axis=0).astype(np.float32)\n",
    "        label = np.concatenate(label, axis=0).astype(np.int32)\n",
    "        score = np.concatenate(score, axis=0).astype(np.float32)\n",
    "        return bbox, label, score\n",
    "\n",
    "    @nograd\n",
    "    def predict(self, imgs,sizes=None,visualize=False):\n",
    "        self.eval()\n",
    "        if visualize:\n",
    "            self.use_preset('visualize')\n",
    "            prepared_imgs = list()\n",
    "            sizes = list()\n",
    "            for img in imgs:\n",
    "                size = img.shape[1:]\n",
    "                img = preprocess(at.tonumpy(img))\n",
    "                prepared_imgs.append(img)\n",
    "                sizes.append(size)\n",
    "        else:\n",
    "             prepared_imgs = imgs \n",
    "        bboxes = list()\n",
    "        labels = list()\n",
    "        scores = list()\n",
    "        for img, size in zip(prepared_imgs, sizes):\n",
    "            img = at.totensor(img[None]).float()\n",
    "            scale = img.shape[3] / size[1]\n",
    "            roi_cls_loc, roi_scores, rois, _ = self(img, scale=scale)\n",
    "            # We are assuming that batch size is 1.\n",
    "            roi_score = roi_scores.data\n",
    "            roi_cls_loc = roi_cls_loc.data\n",
    "            roi = at.totensor(rois) / scale\n",
    "\n",
    "            # Convert predictions to bounding boxes in image coordinates.\n",
    "            # Bounding boxes are scaled to the scale of the input images.\n",
    "            mean = t.Tensor(self.loc_normalize_mean).cuda(). \\\n",
    "                repeat(self.n_class)[None]\n",
    "            std = t.Tensor(self.loc_normalize_std).cuda(). \\\n",
    "                repeat(self.n_class)[None]\n",
    "\n",
    "            roi_cls_loc = (roi_cls_loc * std + mean)\n",
    "            roi_cls_loc = roi_cls_loc.view(-1, self.n_class, 4)\n",
    "            roi = roi.view(-1, 1, 4).expand_as(roi_cls_loc)\n",
    "            cls_bbox = loc2bbox(at.tonumpy(roi).reshape((-1, 4)),\n",
    "                                at.tonumpy(roi_cls_loc).reshape((-1, 4)))\n",
    "            cls_bbox = at.totensor(cls_bbox)\n",
    "            cls_bbox = cls_bbox.view(-1, self.n_class * 4)\n",
    "            # clip bounding box\n",
    "            cls_bbox[:, 0::2] = (cls_bbox[:, 0::2]).clamp(min=0, max=size[0])\n",
    "            cls_bbox[:, 1::2] = (cls_bbox[:, 1::2]).clamp(min=0, max=size[1])\n",
    "\n",
    "            prob = at.tonumpy(F.softmax(at.totensor(roi_score), dim=1))\n",
    "\n",
    "            raw_cls_bbox = at.tonumpy(cls_bbox)\n",
    "            raw_prob = at.tonumpy(prob)\n",
    "\n",
    "            bbox, label, score = self._suppress(raw_cls_bbox, raw_prob)\n",
    "            bboxes.append(bbox)\n",
    "            labels.append(label)\n",
    "            scores.append(score)\n",
    "\n",
    "        self.use_preset('evaluate')\n",
    "        self.train()\n",
    "        return bboxes, labels, scores\n",
    "\n",
    "    def get_optimizer(self):\n",
    "        \"\"\"\n",
    "        return optimizer, It could be overwriten if you want to specify \n",
    "        special optimizer\n",
    "        \"\"\"\n",
    "        lr = opt.lr\n",
    "        params = []\n",
    "        for key, value in dict(self.named_parameters()).items():\n",
    "            if value.requires_grad:\n",
    "                if 'bias' in key:\n",
    "                    params += [{'params': [value], 'lr': lr * 2, 'weight_decay': 0}]\n",
    "                else:\n",
    "                    params += [{'params': [value], 'lr': lr, 'weight_decay': opt.weight_decay}]\n",
    "        if opt.use_adam:\n",
    "            self.optimizer = t.optim.Adam(params)\n",
    "        else:\n",
    "            self.optimizer = t.optim.SGD(params, momentum=0.9)\n",
    "        return self.optimizer\n",
    "\n",
    "    def scale_lr(self, decay=0.1):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] *= decay\n",
    "        return self.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterRCNNVGG16(FasterRCNN):\n",
    "    \"\"\"Faster R-CNN based on VGG-16.\n",
    "    For descriptions on the interface of this model, please refer to\n",
    "    :class:`model.faster_rcnn.FasterRCNN`.\n",
    "    Args:\n",
    "        n_fg_class (int): The number of classes excluding the background.\n",
    "        ratios (list of floats): This is ratios of width to height of\n",
    "            the anchors.\n",
    "        anchor_scales (list of numbers): This is areas of anchors.\n",
    "            Those areas will be the product of the square of an element in\n",
    "            :obj:`anchor_scales` and the original area of the reference\n",
    "            window.\n",
    "    \"\"\"\n",
    "\n",
    "    feat_stride = 16  # downsample 16x for output of conv5 in vgg16\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_fg_class=20,\n",
    "                 ratios=[0.5, 1, 2],\n",
    "                 anchor_scales=[8, 16, 32]\n",
    "                 ):    \n",
    "        extractor, classifier = decom_vgg16()\n",
    "\n",
    "        rpn = RegionProposalNetwork(\n",
    "            512, 512,\n",
    "            ratios=ratios,\n",
    "            anchor_scales=anchor_scales,\n",
    "            feat_stride=self.feat_stride,\n",
    "        )\n",
    "\n",
    "        head = VGG16RoIHead(\n",
    "            n_class=n_fg_class + 1,\n",
    "            roi_size=7,\n",
    "            spatial_scale=(1. / self.feat_stride),\n",
    "            classifier=classifier\n",
    "        )\n",
    "\n",
    "        super(FasterRCNNVGG16, self).__init__(\n",
    "            extractor,\n",
    "            rpn,\n",
    "            head,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_kernel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-ec1854a35ad9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmdoel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFasterRCNNVGG16\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-5359c007d971>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_fg_class, ratios, anchor_scales)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mroi_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mspatial_scale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeat_stride\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         )\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-2a80f8429000>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_class, roi_size, spatial_scale, classifier)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroi_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroi_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial_scale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspatial_scale\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRoIPooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroi_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroi_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial_scale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrois\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroi_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-623937eb43b1>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, outh, outw, spatial_scale)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspatial_scale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRoIPooling2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRoI\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRoI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspatial_scale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrois\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-43-aa9b23cedffb>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, outh, outw, spatial_scale)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mRoI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFunction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspatial_scale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_kernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'roi_forward'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_forward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_kernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'roi_backward'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_backward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mouth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial_scale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mouth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspatial_scale\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'load_kernel' is not defined"
     ]
    }
   ],
   "source": [
    "mdoel = FasterRCNNVGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
