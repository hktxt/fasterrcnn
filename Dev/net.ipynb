{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from torch import nn\n",
    "import six\n",
    "from six import __init__\n",
    "from model.utils.bbox_tools import bbox2loc, bbox_iou, loc2bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anchor_base(base_size=16, ratios=[0.5, 1, 2],\n",
    "                         anchor_scales=[8, 16, 32]):\n",
    "    \"\"\"Generate anchor base windows by enumerating aspect ratio and scales.\n",
    "    Generate anchors that are scaled and modified to the given aspect ratios.\n",
    "    Area of a scaled anchor is preserved when modifying to the given aspect\n",
    "    ratio.\n",
    "    :obj:`R = len(ratios) * len(anchor_scales)` anchors are generated by this\n",
    "    function.\n",
    "    The :obj:`i * len(anchor_scales) + j` th anchor corresponds to an anchor\n",
    "    generated by :obj:`ratios[i]` and :obj:`anchor_scales[j]`.\n",
    "    For example, if the scale is :math:`8` and the ratio is :math:`0.25`,\n",
    "    the width and the height of the base window will be stretched by :math:`8`.\n",
    "    For modifying the anchor to the given aspect ratio,\n",
    "    the height is halved and the width is doubled.\n",
    "    Args:\n",
    "        base_size (number): The width and the height of the reference window.\n",
    "        ratios (list of floats): This is ratios of width to height of\n",
    "            the anchors.\n",
    "        anchor_scales (list of numbers): This is areas of anchors.\n",
    "            Those areas will be the product of the square of an element in\n",
    "            :obj:`anchor_scales` and the original area of the reference\n",
    "            window.\n",
    "    Returns:\n",
    "        ~numpy.ndarray:\n",
    "        An array of shape :math:`(R, 4)`.\n",
    "        Each element is a set of coordinates of a bounding box.\n",
    "        The second axis corresponds to\n",
    "        :math:`(y_{min}, x_{min}, y_{max}, x_{max})` of a bounding box.\n",
    "    \"\"\"\n",
    "    py = base_size / 2.\n",
    "    px = base_size / 2.\n",
    "\n",
    "    anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4),\n",
    "                           dtype=np.float32)\n",
    "    for i in six.moves.range(len(ratios)):\n",
    "        for j in six.moves.range(len(anchor_scales)):\n",
    "            h = base_size * anchor_scales[j] * np.sqrt(ratios[i])\n",
    "            w = base_size * anchor_scales[j] * np.sqrt(1. / ratios[i])\n",
    "\n",
    "            index = i * len(anchor_scales) + j\n",
    "            anchor_base[index, 0] = py - h / 2.\n",
    "            anchor_base[index, 1] = px - w / 2.\n",
    "            anchor_base[index, 2] = py + h / 2.\n",
    "            anchor_base[index, 3] = px + w / 2.\n",
    "    return anchor_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -37.254833,  -82.50967 ,   53.254833,   98.50967 ],\n",
       "       [ -82.50967 , -173.01933 ,   98.50967 ,  189.01933 ],\n",
       "       [-173.01933 , -354.03867 ,  189.01933 ,  370.03867 ],\n",
       "       [ -56.      ,  -56.      ,   72.      ,   72.      ],\n",
       "       [-120.      , -120.      ,  136.      ,  136.      ],\n",
       "       [-248.      , -248.      ,  264.      ,  264.      ],\n",
       "       [ -82.50967 ,  -37.254833,   98.50967 ,   53.254833],\n",
       "       [-173.01933 ,  -82.50967 ,  189.01933 ,   98.50967 ],\n",
       "       [-354.03867 , -173.01933 ,  370.03867 ,  189.01933 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_base = generate_anchor_base();anchor_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _enumerate_shifted_anchor(anchor_base, feat_stride, height, width):\n",
    "    # Enumerate all shifted anchors:\n",
    "    #\n",
    "    # add A anchors (1, A, 4) to\n",
    "    # cell K shifts (K, 1, 4) to get\n",
    "    # shift anchors (K, A, 4)\n",
    "    # reshape to (K*A, 4) shifted anchors\n",
    "    # return (K*A, 4)\n",
    "\n",
    "    # !TODO: add support for torch.CudaTensor\n",
    "    # xp = cuda.get_array_module(anchor_base)\n",
    "    # it seems that it can't be boosed using GPU\n",
    "    import numpy as xp\n",
    "    shift_y = xp.arange(0, height * feat_stride, feat_stride)\n",
    "    shift_x = xp.arange(0, width * feat_stride, feat_stride)\n",
    "    shift_x, shift_y = xp.meshgrid(shift_x, shift_y)\n",
    "    shift = xp.stack((shift_y.ravel(), shift_x.ravel(),\n",
    "                      shift_y.ravel(), shift_x.ravel()), axis=1)\n",
    "\n",
    "    A = anchor_base.shape[0]\n",
    "    K = shift.shape[0]\n",
    "    anchor = anchor_base.reshape((1, A, 4)) + \\\n",
    "             shift.reshape((1, K, 4)).transpose((1, 0, 2))\n",
    "    anchor = anchor.reshape((K * A, 4)).astype(np.float32)\n",
    "    return anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -37.254833,  -82.50967 ,   53.254833,   98.50967 ],\n",
       "       [ -82.50967 , -173.01933 ,   98.50967 ,  189.01933 ],\n",
       "       [-173.01933 , -354.03867 ,  189.01933 ,  370.03867 ],\n",
       "       ...,\n",
       "       [ 701.49036 ,  746.7452  ,  882.50964 ,  837.2548  ],\n",
       "       [ 610.98065 ,  701.49036 ,  973.01935 ,  882.50964 ],\n",
       "       [ 429.96133 ,  610.98065 , 1154.0387  ,  973.01935 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = torch.Tensor(1,512,50,50)\n",
    "n, _, hh, ww = feature.shape\n",
    "anchor = _enumerate_shifted_anchor(\n",
    "            anchor_base,\n",
    "            16, hh, ww)\n",
    "anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22500, 4)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor.shape # 50*50*9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_anchor = anchor.shape[0] // (hh * ww);n_anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(512, 512, 3, 1, 1)\n",
    "score = nn.Conv2d(512, 9 * 2, 1, 1, 0)\n",
    "loc = nn.Conv2d(512, 9 * 4, 1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 50, 50])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = F.relu(conv1(feature));h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 36, 50, 50])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpn_locs = loc(h);rpn_locs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22500, 4])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpn_locs = rpn_locs.permute(0, 2, 3, 1).contiguous().view(n, -1, 4);rpn_locs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 50, 50])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpn_scores = score(h);rpn_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 50, 18])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpn_scores = rpn_scores.permute(0, 2, 3, 1).contiguous();rpn_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 50, 9, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpn_scores.view(n, hh, ww, n_anchor, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 50, 9, 2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpn_softmax_scores = F.softmax(rpn_scores.view(n, hh, ww, n_anchor, 2), dim=4);rpn_softmax_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 50, 9])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpn_fg_scores = rpn_softmax_scores[:, :, :, :, 1].contiguous();rpn_fg_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22500])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpn_fg_scores = rpn_fg_scores.view(n, -1);rpn_fg_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22500, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpn_scores = rpn_scores.view(n, -1, 2);rpn_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22500, 4])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpn_locs[0].cpu().data.numpy();rpn_locs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22500])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpn_fg_scores[0].cpu().data.numpy();rpn_fg_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc2bbox(src_bbox, loc):\n",
    "    import numpy as xp\n",
    "    \"\"\"Decode bounding boxes from bounding box offsets and scales.\n",
    "    Given bounding box offsets and scales computed by\n",
    "    :meth:`bbox2loc`, this function decodes the representation to\n",
    "    coordinates in 2D image coordinates.\n",
    "    Given scales and offsets :math:`t_y, t_x, t_h, t_w` and a bounding\n",
    "    box whose center is :math:`(y, x) = p_y, p_x` and size :math:`p_h, p_w`,\n",
    "    the decoded bounding box's center :math:`\\\\hat{g}_y`, :math:`\\\\hat{g}_x`\n",
    "    and size :math:`\\\\hat{g}_h`, :math:`\\\\hat{g}_w` are calculated\n",
    "    by the following formulas.\n",
    "    * :math:`\\\\hat{g}_y = p_h t_y + p_y`\n",
    "    * :math:`\\\\hat{g}_x = p_w t_x + p_x`\n",
    "    * :math:`\\\\hat{g}_h = p_h \\\\exp(t_h)`\n",
    "    * :math:`\\\\hat{g}_w = p_w \\\\exp(t_w)`\n",
    "    The decoding formulas are used in works such as R-CNN [#]_.\n",
    "    The output is same type as the type of the inputs.\n",
    "    .. [#] Ross Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik. \\\n",
    "    Rich feature hierarchies for accurate object detection and semantic \\\n",
    "    segmentation. CVPR 2014.\n",
    "    Args:\n",
    "        src_bbox (array): A coordinates of bounding boxes.\n",
    "            Its shape is :math:`(R, 4)`. These coordinates are\n",
    "            :math:`p_{ymin}, p_{xmin}, p_{ymax}, p_{xmax}`.\n",
    "        loc (array): An array with offsets and scales.\n",
    "            The shapes of :obj:`src_bbox` and :obj:`loc` should be same.\n",
    "            This contains values :math:`t_y, t_x, t_h, t_w`.\n",
    "    Returns:\n",
    "        array:\n",
    "        Decoded bounding box coordinates. Its shape is :math:`(R, 4)`. \\\n",
    "        The second axis contains four values \\\n",
    "        :math:`\\\\hat{g}_{ymin}, \\\\hat{g}_{xmin},\n",
    "        \\\\hat{g}_{ymax}, \\\\hat{g}_{xmax}`.\n",
    "    \"\"\"\n",
    "\n",
    "    if src_bbox.shape[0] == 0:\n",
    "        return xp.zeros((0, 4), dtype=loc.dtype)\n",
    "\n",
    "    src_bbox = src_bbox.astype(src_bbox.dtype, copy=False) # (22500, 4)\n",
    "\n",
    "    src_height = src_bbox[:, 2] - src_bbox[:, 0] # (22500,)\n",
    "    src_width = src_bbox[:, 3] - src_bbox[:, 1] # (22500,)\n",
    "    src_ctr_y = src_bbox[:, 0] + 0.5 * src_height # (22500,)\n",
    "    src_ctr_x = src_bbox[:, 1] + 0.5 * src_width # (22500,)\n",
    "\n",
    "    dy = loc[:, 0::4] # (22500, 1)\n",
    "    dx = loc[:, 1::4] # (22500, 1)\n",
    "    dh = loc[:, 2::4] # (22500, 1)\n",
    "    dw = loc[:, 3::4] # (22500, 1)\n",
    "\n",
    "    ctr_y = dy * src_height[:, xp.newaxis] + src_ctr_y[:, xp.newaxis] # (22500, 1)\n",
    "    ctr_x = dx * src_width[:, xp.newaxis] + src_ctr_x[:, xp.newaxis] # (22500, 1)\n",
    "    h = xp.exp(dh) * src_height[:, xp.newaxis] # (22500, 1)\n",
    "    w = xp.exp(dw) * src_width[:, xp.newaxis] # (22500, 1)\n",
    "\n",
    "    dst_bbox = xp.zeros(loc.shape, dtype=loc.dtype)\n",
    "    dst_bbox[:, 0::4] = ctr_y - 0.5 * h\n",
    "    dst_bbox[:, 1::4] = ctr_x - 0.5 * w\n",
    "    dst_bbox[:, 2::4] = ctr_y + 0.5 * h\n",
    "    dst_bbox[:, 3::4] = ctr_x + 0.5 * w\n",
    "\n",
    "    return dst_bbox # (22500, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22500, 4)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(anchor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500,) (22500,) (22500,) (22500,)\n"
     ]
    }
   ],
   "source": [
    "src_bbox = anchor\n",
    "src_height = src_bbox[:, 2] - src_bbox[:, 0]\n",
    "src_width = src_bbox[:, 3] - src_bbox[:, 1]\n",
    "src_ctr_y = src_bbox[:, 0] + 0.5 * src_height\n",
    "src_ctr_x = src_bbox[:, 1] + 0.5 * src_width\n",
    "print(src_height.shape, src_width.shape, src_ctr_y.shape, src_ctr_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22500, 4)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpn_locst = rpn_locst[0];rpn_locst.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 1) (22500, 1) (22500, 1) (22500, 1)\n"
     ]
    }
   ],
   "source": [
    "loc = rpn_locst\n",
    "dy = loc[:, 0::4]\n",
    "dx = loc[:, 1::4]\n",
    "dh = loc[:, 2::4]\n",
    "dw = loc[:, 3::4]\n",
    "print(dy.shape, dx.shape,dh.shape,dw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 1) (22500, 1)\n"
     ]
    }
   ],
   "source": [
    "ctr_y = dy * src_height[:, np.newaxis] + src_ctr_y[:, np.newaxis]\n",
    "ctr_x = dx * src_width[:, np.newaxis] + src_ctr_x[:, np.newaxis]\n",
    "print(ctr_y.shape, ctr_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 1) (22500, 1)\n"
     ]
    }
   ],
   "source": [
    "h = np.exp(dh) * src_height[:, np.newaxis]\n",
    "w = np.exp(dw) * src_width[:, np.newaxis]\n",
    "print(h.shape, w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22500, 4)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roi = loc2bbox(anchor, rpn_locst);roi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProposalCreator:\n",
    "    # unNOTE: I'll make it undifferential\n",
    "    # unTODO: make sure it's ok\n",
    "    # It's ok\n",
    "    \"\"\"Proposal regions are generated by calling this object.\n",
    "    The :meth:`__call__` of this object outputs object detection proposals by\n",
    "    applying estimated bounding box offsets\n",
    "    to a set of anchors.\n",
    "    This class takes parameters to control number of bounding boxes to\n",
    "    pass to NMS and keep after NMS.\n",
    "    If the paramters are negative, it uses all the bounding boxes supplied\n",
    "    or keep all the bounding boxes returned by NMS.\n",
    "    This class is used for Region Proposal Networks introduced in\n",
    "    Faster R-CNN [#]_.\n",
    "    .. [#] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. \\\n",
    "    Faster R-CNN: Towards Real-Time Object Detection with \\\n",
    "    Region Proposal Networks. NIPS 2015.\n",
    "    Args:\n",
    "        nms_thresh (float): Threshold value used when calling NMS.\n",
    "        n_train_pre_nms (int): Number of top scored bounding boxes\n",
    "            to keep before passing to NMS in train mode.\n",
    "        n_train_post_nms (int): Number of top scored bounding boxes\n",
    "            to keep after passing to NMS in train mode.\n",
    "        n_test_pre_nms (int): Number of top scored bounding boxes\n",
    "            to keep before passing to NMS in test mode.\n",
    "        n_test_post_nms (int): Number of top scored bounding boxes\n",
    "            to keep after passing to NMS in test mode.\n",
    "        force_cpu_nms (bool): If this is :obj:`True`,\n",
    "            always use NMS in CPU mode. If :obj:`False`,\n",
    "            the NMS mode is selected based on the type of inputs.\n",
    "        min_size (int): A paramter to determine the threshold on\n",
    "            discarding bounding boxes based on their sizes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 parent_model,\n",
    "                 nms_thresh=0.7,\n",
    "                 n_train_pre_nms=12000,\n",
    "                 n_train_post_nms=2000,\n",
    "                 n_test_pre_nms=6000,\n",
    "                 n_test_post_nms=300,\n",
    "                 min_size=16\n",
    "                 ):\n",
    "        self.parent_model = parent_model\n",
    "        self.nms_thresh = nms_thresh\n",
    "        self.n_train_pre_nms = n_train_pre_nms\n",
    "        self.n_train_post_nms = n_train_post_nms\n",
    "        self.n_test_pre_nms = n_test_pre_nms\n",
    "        self.n_test_post_nms = n_test_post_nms\n",
    "        self.min_size = min_size\n",
    "\n",
    "    def __call__(self, loc, score,\n",
    "                 anchor, img_size, scale=1.):\n",
    "        \"\"\"input should  be ndarray\n",
    "        Propose RoIs.\n",
    "        Inputs :obj:`loc, score, anchor` refer to the same anchor when indexed\n",
    "        by the same index.\n",
    "        On notations, :math:`R` is the total number of anchors. This is equal\n",
    "        to product of the height and the width of an image and the number of\n",
    "        anchor bases per pixel.\n",
    "        Type of the output is same as the inputs.\n",
    "        Args:\n",
    "            loc (array): Predicted offsets and scaling to anchors.\n",
    "                Its shape is :math:`(R, 4)`.\n",
    "            score (array): Predicted foreground probability for anchors.\n",
    "                Its shape is :math:`(R,)`.\n",
    "            anchor (array): Coordinates of anchors. Its shape is\n",
    "                :math:`(R, 4)`.\n",
    "            img_size (tuple of ints): A tuple :obj:`height, width`,\n",
    "                which contains image size after scaling.\n",
    "            scale (float): The scaling factor used to scale an image after\n",
    "                reading it from a file.\n",
    "        Returns:\n",
    "            array:\n",
    "            An array of coordinates of proposal boxes.\n",
    "            Its shape is :math:`(S, 4)`. :math:`S` is less than\n",
    "            :obj:`self.n_test_post_nms` in test time and less than\n",
    "            :obj:`self.n_train_post_nms` in train time. :math:`S` depends on\n",
    "            the size of the predicted bounding boxes and the number of\n",
    "            bounding boxes discarded by NMS.\n",
    "        \"\"\"\n",
    "        # NOTE: when test, remember\n",
    "        # faster_rcnn.eval()\n",
    "        # to set self.traing = False\n",
    "        if self.parent_model.training:\n",
    "            n_pre_nms = self.n_train_pre_nms\n",
    "            n_post_nms = self.n_train_post_nms\n",
    "        else:\n",
    "            n_pre_nms = self.n_test_pre_nms\n",
    "            n_post_nms = self.n_test_post_nms\n",
    "\n",
    "        # Convert anchors into proposal via bbox transformations.\n",
    "        # roi = loc2bbox(anchor, loc)\n",
    "        roi = loc2bbox(anchor, loc)\n",
    "\n",
    "        # Clip predicted boxes to image.\n",
    "        roi[:, slice(0, 4, 2)] = np.clip(\n",
    "            roi[:, slice(0, 4, 2)], 0, img_size[0])\n",
    "        roi[:, slice(1, 4, 2)] = np.clip(\n",
    "            roi[:, slice(1, 4, 2)], 0, img_size[1])\n",
    "\n",
    "        # Remove predicted boxes with either height or width < threshold.\n",
    "        min_size = self.min_size * scale\n",
    "        hs = roi[:, 2] - roi[:, 0]\n",
    "        ws = roi[:, 3] - roi[:, 1]\n",
    "        keep = np.where((hs >= min_size) & (ws >= min_size))[0]\n",
    "        roi = roi[keep, :]\n",
    "        score = score[keep]\n",
    "\n",
    "        # Sort all (proposal, score) pairs by score from highest to lowest.\n",
    "        # Take top pre_nms_topN (e.g. 6000).\n",
    "        order = score.ravel().argsort()[::-1]\n",
    "        if n_pre_nms > 0:\n",
    "            order = order[:n_pre_nms]\n",
    "        roi = roi[order, :]\n",
    "\n",
    "        # Apply nms (e.g. threshold = 0.7).\n",
    "        # Take after_nms_topN (e.g. 300).\n",
    "\n",
    "        # unNOTE: somthing is wrong here!\n",
    "        # TODO: remove cuda.to_gpu\n",
    "        keep = non_maximum_suppression(\n",
    "            cp.ascontiguousarray(cp.asarray(roi)),\n",
    "            thresh=self.nms_thresh)\n",
    "        if n_post_nms > 0:\n",
    "            keep = keep[:n_post_nms]\n",
    "        roi = roi[keep]\n",
    "        return roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionProposalNetwork(nn.Module):\n",
    "    \"\"\"Region Proposal Network introduced in Faster R-CNN.\n",
    "\n",
    "    This is Region Proposal Network introduced in Faster R-CNN [#]_.\n",
    "    This takes features extracted from images and propose\n",
    "    class agnostic bounding boxes around \"objects\".\n",
    "\n",
    "    .. [#] Shaoqing Ren, Kaiming He, Ross Girshick, Jian Sun. \\\n",
    "    Faster R-CNN: Towards Real-Time Object Detection with \\\n",
    "    Region Proposal Networks. NIPS 2015.\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): The channel size of input.\n",
    "        mid_channels (int): The channel size of the intermediate tensor.\n",
    "        ratios (list of floats): This is ratios of width to height of\n",
    "            the anchors.\n",
    "        anchor_scales (list of numbers): This is areas of anchors.\n",
    "            Those areas will be the product of the square of an element in\n",
    "            :obj:`anchor_scales` and the original area of the reference\n",
    "            window.\n",
    "        feat_stride (int): Stride size after extracting features from an\n",
    "            image.\n",
    "        initialW (callable): Initial weight value. If :obj:`None` then this\n",
    "            function uses Gaussian distribution scaled by 0.1 to\n",
    "            initialize weight.\n",
    "            May also be a callable that takes an array and edits its values.\n",
    "        proposal_creator_params (dict): Key valued paramters for\n",
    "            :class:`model.utils.creator_tools.ProposalCreator`.\n",
    "\n",
    "    .. seealso::\n",
    "        :class:`~model.utils.creator_tools.ProposalCreator`\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, in_channels=512, mid_channels=512, ratios=[0.5, 1, 2],\n",
    "            anchor_scales=[8, 16, 32], feat_stride=16,\n",
    "            proposal_creator_params=dict(),\n",
    "    ):\n",
    "        super(RegionProposalNetwork, self).__init__()\n",
    "        self.anchor_base = generate_anchor_base(\n",
    "            anchor_scales=anchor_scales, ratios=ratios)\n",
    "        self.feat_stride = feat_stride\n",
    "        self.proposal_layer = ProposalCreator(self, **proposal_creator_params)\n",
    "        n_anchor = self.anchor_base.shape[0]\n",
    "        self.conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1)\n",
    "        self.score = nn.Conv2d(mid_channels, n_anchor * 2, 1, 1, 0)\n",
    "        self.loc = nn.Conv2d(mid_channels, n_anchor * 4, 1, 1, 0)\n",
    "        normal_init(self.conv1, 0, 0.01)\n",
    "        normal_init(self.score, 0, 0.01)\n",
    "        normal_init(self.loc, 0, 0.01)\n",
    "\n",
    "    def forward(self, x, img_size, scale=1.):\n",
    "        \"\"\"Forward Region Proposal Network.\n",
    "\n",
    "        Here are notations.\n",
    "\n",
    "        * :math:`N` is batch size.\n",
    "        * :math:`C` channel size of the input.\n",
    "        * :math:`H` and :math:`W` are height and witdh of the input feature.\n",
    "        * :math:`A` is number of anchors assigned to each pixel.\n",
    "\n",
    "        Args:\n",
    "            x (~torch.autograd.Variable): The Features extracted from images.\n",
    "                Its shape is :math:`(N, C, H, W)`.\n",
    "            img_size (tuple of ints): A tuple :obj:`height, width`,\n",
    "                which contains image size after scaling.\n",
    "            scale (float): The amount of scaling done to the input images after\n",
    "                reading them from files.\n",
    "\n",
    "        Returns:\n",
    "            (~torch.autograd.Variable, ~torch.autograd.Variable, array, array, array):\n",
    "\n",
    "            This is a tuple of five following values.\n",
    "\n",
    "            * **rpn_locs**: Predicted bounding box offsets and scales for \\\n",
    "                anchors. Its shape is :math:`(N, H W A, 4)`.\n",
    "            * **rpn_scores**:  Predicted foreground scores for \\\n",
    "                anchors. Its shape is :math:`(N, H W A, 2)`.\n",
    "            * **rois**: A bounding box array containing coordinates of \\\n",
    "                proposal boxes.  This is a concatenation of bounding box \\\n",
    "                arrays from multiple images in the batch. \\\n",
    "                Its shape is :math:`(R', 4)`. Given :math:`R_i` predicted \\\n",
    "                bounding boxes from the :math:`i` th image, \\\n",
    "                :math:`R' = \\\\sum _{i=1} ^ N R_i`.\n",
    "            * **roi_indices**: An array containing indices of images to \\\n",
    "                which RoIs correspond to. Its shape is :math:`(R',)`.\n",
    "            * **anchor**: Coordinates of enumerated shifted anchors. \\\n",
    "                Its shape is :math:`(H W A, 4)`.\n",
    "\n",
    "        \"\"\"\n",
    "        n, _, hh, ww = x.shape # bactch,channel, h, w \n",
    "        anchor = _enumerate_shifted_anchor(\n",
    "            np.array(self.anchor_base),\n",
    "            self.feat_stride, hh, ww)\n",
    "        # (22500, 4)\n",
    "\n",
    "        n_anchor = anchor.shape[0] // (hh * ww) # 9\n",
    "        h = F.relu(self.conv1(x)) # torch.Size([1, 512, 50, 50])\n",
    "\n",
    "        rpn_locs = self.loc(h) # torch.Size([1, 36, 50, 50])\n",
    "        # UNNOTE: check whether need contiguous\n",
    "        # A: Yes\n",
    "        rpn_locs = rpn_locs.permute(0, 2, 3, 1).contiguous().view(n, -1, 4) # torch.Size([1, 22500, 4])\n",
    "        rpn_scores = self.score(h) # torch.Size([1, 18, 50, 50])\n",
    "        rpn_scores = rpn_scores.permute(0, 2, 3, 1).contiguous() # torch.Size([1, 50, 50, 18])\n",
    "        rpn_softmax_scores = F.softmax(rpn_scores.view(n, hh, ww, n_anchor, 2), dim=4) # torch.Size([1, 50, 50, 9, 2])\n",
    "        rpn_fg_scores = rpn_softmax_scores[:, :, :, :, 1].contiguous() # torch.Size([1, 50, 50, 9])\n",
    "        rpn_fg_scores = rpn_fg_scores.view(n, -1) # torch.Size([1, 22500])\n",
    "        rpn_scores = rpn_scores.view(n, -1, 2) # torch.Size([1, 22500, 2])\n",
    "\n",
    "        rois = list()\n",
    "        roi_indices = list()\n",
    "        for i in range(n): # n batches\n",
    "            roi = self.proposal_layer(\n",
    "                rpn_locs[i].cpu().data.numpy(), # torch.Size([1,22500, 4])\n",
    "                rpn_fg_scores[i].cpu().data.numpy(), # torch.Size([1, 22500])\n",
    "                anchor, img_size,\n",
    "                scale=scale)\n",
    "            # (22500, 4)\n",
    "            batch_index = i * np.ones((len(roi),), dtype=np.int32)\n",
    "            rois.append(roi)\n",
    "            roi_indices.append(batch_index)\n",
    "\n",
    "        rois = np.concatenate(rois, axis=0) # (22500*n, 4)\n",
    "        roi_indices = np.concatenate(roi_indices, axis=0) # (22500*n, 4)\n",
    "        return rpn_locs, rpn_scores, rois, roi_indices, anchor\n",
    "\n",
    "\n",
    "def _enumerate_shifted_anchor(anchor_base, feat_stride, height, width):\n",
    "    # Enumerate all shifted anchors:\n",
    "    #\n",
    "    # add A anchors (1, A, 4) to\n",
    "    # cell K shifts (K, 1, 4) to get\n",
    "    # shift anchors (K, A, 4)\n",
    "    # reshape to (K*A, 4) shifted anchors\n",
    "    # return (K*A, 4)\n",
    "\n",
    "    # !TODO: add support for torch.CudaTensor\n",
    "    # xp = cuda.get_array_module(anchor_base)\n",
    "    # it seems that it can't be boosed using GPU\n",
    "    import numpy as xp\n",
    "    shift_y = xp.arange(0, height * feat_stride, feat_stride)\n",
    "    shift_x = xp.arange(0, width * feat_stride, feat_stride)\n",
    "    shift_x, shift_y = xp.meshgrid(shift_x, shift_y)\n",
    "    shift = xp.stack((shift_y.ravel(), shift_x.ravel(),\n",
    "                      shift_y.ravel(), shift_x.ravel()), axis=1)\n",
    "\n",
    "    A = anchor_base.shape[0]\n",
    "    K = shift.shape[0]\n",
    "    anchor = anchor_base.reshape((1, A, 4)) + \\\n",
    "             shift.reshape((1, K, 4)).transpose((1, 0, 2))\n",
    "    anchor = anchor.reshape((K * A, 4)).astype(np.float32)\n",
    "    return anchor\n",
    "\n",
    "\n",
    "def normal_init(m, mean, stddev, truncated=False):\n",
    "    \"\"\"\n",
    "    weight initalizer: truncated normal and random normal.\n",
    "    \"\"\"\n",
    "    # x is a parameter\n",
    "    if truncated:\n",
    "        m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean)  # not a perfect approximation\n",
    "    else:\n",
    "        m.weight.data.normal_(mean, stddev)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegionProposalNetwork(\n",
       "  (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (score): Conv2d(512, 18, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (loc): Conv2d(512, 36, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpn = RegionProposalNetwork(\n",
    "            512, 512,\n",
    "            ratios=[0.5, 1, 2],\n",
    "            anchor_scales=[8, 16, 32],\n",
    "            feat_stride=16,\n",
    "        );rpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18 = 2*9\n",
    "# 36 = 4*9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16RoIHead(nn.Module):\n",
    "    \"\"\"Faster R-CNN Head for VGG-16 based implementation.\n",
    "    This class is used as a head for Faster R-CNN.\n",
    "    This outputs class-wise localizations and classification based on feature\n",
    "    maps in the given RoIs.\n",
    "    \n",
    "    Args:\n",
    "        n_class (int): The number of classes possibly including the background.\n",
    "        roi_size (int): Height and width of the feature maps after RoI-pooling.\n",
    "        spatial_scale (float): Scale of the roi is resized.\n",
    "        classifier (nn.Module): Two layer Linear ported from vgg16\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_class, roi_size, spatial_scale,\n",
    "                 classifier):\n",
    "        # n_class includes the background\n",
    "        super(VGG16RoIHead, self).__init__()\n",
    "\n",
    "        self.classifier = classifier\n",
    "        self.cls_loc = nn.Linear(4096, n_class * 4)\n",
    "        self.score = nn.Linear(4096, n_class)\n",
    "\n",
    "        normal_init(self.cls_loc, 0, 0.001)\n",
    "        normal_init(self.score, 0, 0.01)\n",
    "\n",
    "        self.n_class = n_class\n",
    "        self.roi_size = roi_size\n",
    "        self.spatial_scale = spatial_scale\n",
    "        self.roi = RoIPooling2D(self.roi_size, self.roi_size, self.spatial_scale)\n",
    "\n",
    "    def forward(self, x, rois, roi_indices):\n",
    "        \"\"\"Forward the chain.\n",
    "        We assume that there are :math:`N` batches.\n",
    "        Args:\n",
    "            x (Variable): 4D image variable.\n",
    "            rois (Tensor): A bounding box array containing coordinates of\n",
    "                proposal boxes.  This is a concatenation of bounding box\n",
    "                arrays from multiple images in the batch.\n",
    "                Its shape is :math:`(R', 4)`. Given :math:`R_i` proposed\n",
    "                RoIs from the :math:`i` th image,\n",
    "                :math:`R' = \\\\sum _{i=1} ^ N R_i`.\n",
    "            roi_indices (Tensor): An array containing indices of images to\n",
    "                which bounding boxes correspond to. Its shape is :math:`(R',)`.\n",
    "        \"\"\"\n",
    "        # in case roi_indices is  ndarray\n",
    "        roi_indices = at.totensor(roi_indices).float()\n",
    "        rois = at.totensor(rois).float()\n",
    "        indices_and_rois = t.cat([roi_indices[:, None], rois], dim=1)\n",
    "        # NOTE: important: yx->xy\n",
    "        xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]]\n",
    "        indices_and_rois =  xy_indices_and_rois.contiguous()\n",
    "\n",
    "        pool = self.roi(x, indices_and_rois)\n",
    "        pool = pool.view(pool.size(0), -1)\n",
    "        fc7 = self.classifier(pool)\n",
    "        roi_cls_locs = self.cls_loc(fc7)\n",
    "        roi_scores = self.score(fc7)\n",
    "        return roi_cls_locs, roi_scores\n",
    "\n",
    "\n",
    "def normal_init(m, mean, stddev, truncated=False):\n",
    "    \"\"\"\n",
    "    weight initalizer: truncated normal and random normal.\n",
    "    \"\"\"\n",
    "    # x is a parameter\n",
    "    if truncated:\n",
    "        m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean)  # not a perfect approximation\n",
    "    else:\n",
    "        m.weight.data.normal_(mean, stddev)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
