{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "jj_5RWJSZYFY",
    "outputId": "de90b061-d38b-4c94-844d-5901cb1a6e1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (41.0.1)\n",
      "Requirement already up-to-date: pip in /usr/local/lib/python3.6/dist-packages (19.1.1)\n"
     ]
    }
   ],
   "source": [
    "pip install -U setuptools pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "7lfRyHrmVgOF",
    "outputId": "3f720e00-9822-4c30-c27d-a84a84385912"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Version 10.0.130\n"
     ]
    }
   ],
   "source": [
    "! cat /usr/local/cuda/version.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "colab_type": "code",
    "id": "xmSwbZ5LZ5AZ",
    "outputId": "5c102993-ed09-4265-efe6-a8149df359c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cupy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/d6/532e5da87f3b513cd0b98bcbf9a58fb6758598039944c42cb93d13b71a5f/cupy-5.4.0.tar.gz (2.5MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5MB 2.8MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy) (1.16.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: fastrlock>=0.3 in /usr/local/lib/python3.6/dist-packages (from cupy) (0.4)\n",
      "Building wheels for collected packages: cupy\n",
      "  Building wheel for cupy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/77/44/1f/138aca0bd8e7dd12d307eb06b595b15de4f92f2223cf95645d\n",
      "Successfully built cupy\n",
      "Installing collected packages: cupy\n",
      "Successfully installed cupy-5.4.0\n"
     ]
    }
   ],
   "source": [
    "pip install -U cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "1PUTVHmUVuGi",
    "outputId": "f6069a7f-1618-4084-b23a-15e315de29e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cupy-cuda100 in /usr/local/lib/python3.6/dist-packages (5.4.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda100) (1.16.3)\n",
      "Requirement already satisfied: fastrlock>=0.3 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda100) (0.4)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda100) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "pip install cupy-cuda100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lEkL0oM59rsx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple\n",
    "from string import Template\n",
    "from torch.autograd.function import once_differentiable\n",
    "from torch.autograd import Function\n",
    "from torch.nn.modules.utils import _pair\n",
    "from roi_cupy import kernel_backward, kernel_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wUDQs3bBCOmO"
   },
   "outputs": [],
   "source": [
    "class _RPN(nn.Module):\n",
    "    \"\"\" region proposal network \"\"\"\n",
    "    def __init__(self, din):\n",
    "        super(_RPN, self).__init__()\n",
    "        \n",
    "        self.din = din  # get depth of input feature map, e.g., 512\n",
    "        self.anchor_scales = [8, 16, 32]\n",
    "        self.anchor_ratios = [0.5, 1, 2]\n",
    "        self.feat_stride = [16]\n",
    "\n",
    "        # define the convrelu layers processing input feature map\n",
    "        self.RPN_Conv = nn.Conv2d(self.din, 512, 3, 1, 1, bias=True)\n",
    "\n",
    "        # define bg/fg classifcation score layer\n",
    "        self.nc_score_out = len(self.anchor_scales) * len(self.anchor_ratios) * 2 # 2(bg/fg) * 9 (anchors)\n",
    "        self.RPN_cls_score = nn.Conv2d(512, self.nc_score_out, 1, 1, 0)\n",
    "\n",
    "        # define anchor box offset prediction layer\n",
    "        self.nc_bbox_out = len(self.anchor_scales) * len(self.anchor_ratios) * 4 # 4(coords) * 9 (anchors)\n",
    "        self.RPN_bbox_pred = nn.Conv2d(512, self.nc_bbox_out, 1, 1, 0)\n",
    "\n",
    "        # define proposal layer\n",
    "        self.RPN_proposal = _ProposalLayer(self.feat_stride, self.anchor_scales, self.anchor_ratios)\n",
    "\n",
    "        # define anchor target layer\n",
    "        self.RPN_anchor_target = _AnchorTargetLayer(self.feat_stride, self.anchor_scales, self.anchor_ratios)\n",
    "\n",
    "        self.rpn_loss_cls = 0\n",
    "        self.rpn_loss_box = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def reshape(x, d):\n",
    "        input_shape = x.size()\n",
    "        x = x.view(\n",
    "            input_shape[0],\n",
    "            int(d),\n",
    "            int(float(input_shape[1] * input_shape[2]) / float(d)),\n",
    "            input_shape[3]\n",
    "        )\n",
    "        return x\n",
    "\n",
    "    def forward(self, base_feat, im_info, gt_boxes, num_boxes):\n",
    "\n",
    "        batch_size = base_feat.size(0)\n",
    "\n",
    "        # return feature map after convrelu layer\n",
    "        rpn_conv1 = F.relu(self.RPN_Conv(base_feat), inplace=True)\n",
    "        # get rpn classification score\n",
    "        rpn_cls_score = self.RPN_cls_score(rpn_conv1)\n",
    "\n",
    "        rpn_cls_score_reshape = self.reshape(rpn_cls_score, 2)\n",
    "        rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape, 1)\n",
    "        rpn_cls_prob = self.reshape(rpn_cls_prob_reshape, self.nc_score_out)\n",
    "\n",
    "        # get rpn offsets to the anchor boxes\n",
    "        rpn_bbox_pred = self.RPN_bbox_pred(rpn_conv1)\n",
    "\n",
    "        # proposal layer\n",
    "        cfg_key = 'TRAIN' if self.training else 'TEST'\n",
    "\n",
    "        rois = self.RPN_proposal((rpn_cls_prob.data, rpn_bbox_pred.data,\n",
    "                                 im_info, cfg_key))\n",
    "\n",
    "        self.rpn_loss_cls = 0\n",
    "        self.rpn_loss_box = 0\n",
    "\n",
    "        # generating training labels and build the rpn loss\n",
    "        if self.training:\n",
    "            assert gt_boxes is not None\n",
    "\n",
    "            rpn_data = self.RPN_anchor_target((rpn_cls_score.data, gt_boxes, im_info, num_boxes))\n",
    "\n",
    "            # compute classification loss\n",
    "            rpn_cls_score = rpn_cls_score_reshape.permute(0, 2, 3, 1).contiguous().view(batch_size, -1, 2)\n",
    "            rpn_label = rpn_data[0].view(batch_size, -1)\n",
    "\n",
    "            rpn_keep = Variable(rpn_label.view(-1).ne(-1).nonzero().view(-1))\n",
    "            rpn_cls_score = torch.index_select(rpn_cls_score.view(-1,2), 0, rpn_keep)\n",
    "            rpn_label = torch.index_select(rpn_label.view(-1), 0, rpn_keep.data)\n",
    "            rpn_label = Variable(rpn_label.long())\n",
    "            self.rpn_loss_cls = F.cross_entropy(rpn_cls_score, rpn_label)\n",
    "            fg_cnt = torch.sum(rpn_label.data.ne(0))\n",
    "\n",
    "            rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights = rpn_data[1:]\n",
    "\n",
    "            # compute bbox regression loss\n",
    "            rpn_bbox_inside_weights = Variable(rpn_bbox_inside_weights)\n",
    "            rpn_bbox_outside_weights = Variable(rpn_bbox_outside_weights)\n",
    "            rpn_bbox_targets = Variable(rpn_bbox_targets)\n",
    "\n",
    "            self.rpn_loss_box = _smooth_l1_loss(rpn_bbox_pred, rpn_bbox_targets, rpn_bbox_inside_weights,\n",
    "                                                            rpn_bbox_outside_weights, sigma=3, dim=[1,2,3])\n",
    "\n",
    "        return rois, self.rpn_loss_cls, self.rpn_loss_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ztOreO-EnFM"
   },
   "outputs": [],
   "source": [
    "def generate_anchors(base_size=16, ratios=[0.5, 1, 2],\n",
    "                     scales=2**np.arange(3, 6)):\n",
    "    \"\"\"\n",
    "    Generate anchor (reference) windows by enumerating aspect ratios X\n",
    "    scales wrt a reference (0, 0, 15, 15) window.\n",
    "    \"\"\"\n",
    "\n",
    "    base_anchor = np.array([1, 1, base_size, base_size]) - 1\n",
    "    ratio_anchors = _ratio_enum(base_anchor, ratios)\n",
    "    anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales)\n",
    "                         for i in range(ratio_anchors.shape[0])])\n",
    "    return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NvWrjApdE710"
   },
   "outputs": [],
   "source": [
    "def _ratio_enum(anchor, ratios):\n",
    "    \"\"\"\n",
    "    Enumerate a set of anchors for each aspect ratio wrt an anchor.\n",
    "    \"\"\"\n",
    "\n",
    "    w, h, x_ctr, y_ctr = _whctrs(anchor)\n",
    "    size = w * h\n",
    "    size_ratios = size / ratios\n",
    "    ws = np.round(np.sqrt(size_ratios))\n",
    "    hs = np.round(ws * ratios)\n",
    "    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n",
    "    return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RfpDP2AaFDMU"
   },
   "outputs": [],
   "source": [
    "def _whctrs(anchor):\n",
    "    \"\"\"\n",
    "    Return width, height, x center, and y center for an anchor (window).\n",
    "    \"\"\"\n",
    "\n",
    "    w = anchor[2] - anchor[0] + 1\n",
    "    h = anchor[3] - anchor[1] + 1\n",
    "    x_ctr = anchor[0] + 0.5 * (w - 1)\n",
    "    y_ctr = anchor[1] + 0.5 * (h - 1)\n",
    "    return w, h, x_ctr, y_ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vp0HnlqcFILE"
   },
   "outputs": [],
   "source": [
    "def _mkanchors(ws, hs, x_ctr, y_ctr):\n",
    "    \"\"\"\n",
    "    Given a vector of widths (ws) and heights (hs) around a center\n",
    "    (x_ctr, y_ctr), output a set of anchors (windows).\n",
    "    \"\"\"\n",
    "\n",
    "    ws = ws[:, np.newaxis]\n",
    "    hs = hs[:, np.newaxis]\n",
    "    anchors = np.hstack((x_ctr - 0.5 * (ws - 1),\n",
    "                         y_ctr - 0.5 * (hs - 1),\n",
    "                         x_ctr + 0.5 * (ws - 1),\n",
    "                         y_ctr + 0.5 * (hs - 1)))\n",
    "    return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sRHEt1ecFfz7"
   },
   "outputs": [],
   "source": [
    "def _scale_enum(anchor, scales):\n",
    "    \"\"\"\n",
    "    Enumerate a set of anchors for each scale wrt an anchor.\n",
    "    \"\"\"\n",
    "\n",
    "    w, h, x_ctr, y_ctr = _whctrs(anchor)\n",
    "    ws = w * scales\n",
    "    hs = h * scales\n",
    "    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n",
    "    return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QsrJC5QNiMu0"
   },
   "outputs": [],
   "source": [
    "def nms(bounding_boxes, confidence_score, threshold):\n",
    "    # If no bounding boxes, return empty list\n",
    "    if len(bounding_boxes) == 0:\n",
    "        return [], []\n",
    "\n",
    "    # Bounding boxes\n",
    "    #boxes = np.array(bounding_boxes)\n",
    "    boxes = bounding_boxes.cpu().detach().numpy()\n",
    "    '''[[187  82 337 317]\n",
    "         [150  67 305 282]\n",
    "         [246 121 368 304]]'''\n",
    "\n",
    "    # coordinates of bounding boxes\n",
    "    start_x = boxes[:, 0]\n",
    "    start_y = boxes[:, 1]\n",
    "    end_x = boxes[:, 2]\n",
    "    end_y = boxes[:, 3]\n",
    "\n",
    "    # Confidence scores of bounding boxes\n",
    "    #score = np.array(confidence_score)\n",
    "    score = confidence_score.cpu().detach().numpy()\n",
    "    '[0.9  0.75 0.8 ]'\n",
    "\n",
    "    # Picked bounding boxes id\n",
    "    picked_idx = []\n",
    "\n",
    "    # Compute areas of bounding boxes\n",
    "    areas = (end_x - start_x + 1) * (end_y - start_y + 1)\n",
    "    '[35636 33696 22632]'\n",
    "\n",
    "    # Sort by confidence score of bounding boxes\n",
    "    order = np.argsort(score)\n",
    "    '[1 2 0]'\n",
    "\n",
    "    # Iterate bounding boxes\n",
    "    while order.size > 0:\n",
    "        # The index of largest confidence score\n",
    "        index = order[-1] # 0, \n",
    "\n",
    "        # Pick the bounding box with largest confidence score\n",
    "        picked_idx.append(index) # [(187, 82, 337, 317)]\n",
    "        # Compute ordinates of intersection-over-union(IOU)\n",
    "        #print(end_y[order[:-1]])\n",
    "        x1 = np.maximum(start_x[index], start_x[order[:-1]]) # 187,[150 246]; \n",
    "        x2 = np.minimum(end_x[index], end_x[order[:-1]]) # 337, [305 368]; \n",
    "        y1 = np.maximum(start_y[index], start_y[order[:-1]]) # 82, [ 67 121]; \n",
    "        y2 = np.minimum(end_y[index], end_y[order[:-1]]) # 317, [282 304]; \n",
    "        #print(x1,x2,y1,y2) [187 246] [305 337] [ 82 121] [282 304]\n",
    "\n",
    "        # Compute areas of intersection-over-union\n",
    "        w = np.maximum(0.0, x2 - x1 + 1) # [119.  92.]\n",
    "        h = np.maximum(0.0, y2 - y1 + 1) # [201. 184.]\n",
    "        intersection = w * h # [23919. 16928.]\n",
    "\n",
    "        # Compute the ratio between intersection and union\n",
    "        ratio = intersection / (areas[index] + areas[order[:-1]] - intersection) # [0.5266994  0.40948234]\n",
    "\n",
    "        left = np.where(ratio < threshold) # (array([], dtype=int64),)\n",
    "        order = order[left]\n",
    "\n",
    "    return torch.from_numpy(np.array(picked_idx)).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RuvGnJbsjwNV"
   },
   "outputs": [],
   "source": [
    "bounding_boxes = np.array([(187, 82, 337, 317), (150, 67, 305, 282), (246, 121, 368, 304)])\n",
    "confidence_score = np.array([0.9, 0.75, 0.8])\n",
    "\n",
    "bounding_boxes = torch.from_numpy(bounding_boxes).to(DEVICE)\n",
    "confidence_score = torch.from_numpy(confidence_score).to(DEVICE)\n",
    "\n",
    "picked_idx = nms(bounding_boxes, confidence_score, 0.5) # 0,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "h21XHN0cjx-n",
    "outputId": "5b93cb87-c035-43cb-b265-1a55e01c3c4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2], device='cuda:0')"
      ]
     },
     "execution_count": 87,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "picked_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VipiColGEYIl"
   },
   "outputs": [],
   "source": [
    "class _ProposalLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Outputs object detection proposals by applying estimated bounding-box\n",
    "    transformations to a set of regular boxes (called \"anchors\").\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feat_stride, scales, ratios):\n",
    "        super(_ProposalLayer, self).__init__()\n",
    "\n",
    "        self._feat_stride = feat_stride\n",
    "        self._anchors = torch.from_numpy(generate_anchors(scales=np.array(scales),\n",
    "            ratios=np.array(ratios))).float()\n",
    "        self._num_anchors = self._anchors.size(0)\n",
    "\n",
    "        # rois blob: holds R regions of interest, each is a 5-tuple\n",
    "        # (n, x1, y1, x2, y2) specifying an image batch index n and a\n",
    "        # rectangle (x1, y1, x2, y2)\n",
    "        # top[0].reshape(1, 5)\n",
    "        #\n",
    "        # # scores blob: holds scores for R regions of interest\n",
    "        # if len(top) > 1:\n",
    "        #     top[1].reshape(1, 1, 1, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        # Algorithm:\n",
    "        #\n",
    "        # for each (H, W) location i\n",
    "        #   generate A anchor boxes centered on cell i\n",
    "        #   apply predicted bbox deltas at cell i to each of the A anchors\n",
    "        # clip predicted boxes to image\n",
    "        # remove predicted boxes with either height or width < threshold\n",
    "        # sort all (proposal, score) pairs by score from highest to lowest\n",
    "        # take top pre_nms_topN proposals before NMS\n",
    "        # apply NMS with threshold 0.7 to remaining proposals\n",
    "        # take after_nms_topN proposals after NMS\n",
    "        # return the top proposals (-> RoIs top, scores top)\n",
    "\n",
    "\n",
    "        # the first set of _num_anchors channels are bg probs\n",
    "        # the second set are the fg probs\n",
    "        scores = input[0][:, self._num_anchors:, :, :]\n",
    "        bbox_deltas = input[1]\n",
    "        im_info = input[2]\n",
    "        cfg_key = input[3]\n",
    "\n",
    "        pre_nms_topN  = 12000\n",
    "        post_nms_topN = 2000\n",
    "        nms_thresh    = 0.7\n",
    "        min_size      = 8\n",
    "\n",
    "        batch_size = bbox_deltas.size(0)\n",
    "\n",
    "        feat_height, feat_width = scores.size(2), scores.size(3)\n",
    "        shift_x = np.arange(0, feat_width) * self._feat_stride\n",
    "        shift_y = np.arange(0, feat_height) * self._feat_stride\n",
    "        shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "        shifts = torch.from_numpy(np.vstack((shift_x.ravel(), shift_y.ravel(),\n",
    "                                  shift_x.ravel(), shift_y.ravel())).transpose())\n",
    "        shifts = shifts.contiguous().type_as(scores).float()\n",
    "\n",
    "        A = self._num_anchors\n",
    "        K = shifts.size(0)\n",
    "\n",
    "        self._anchors = self._anchors.type_as(scores)\n",
    "        # anchors = self._anchors.view(1, A, 4) + shifts.view(1, K, 4).permute(1, 0, 2).contiguous()\n",
    "        anchors = self._anchors.view(1, A, 4) + shifts.view(K, 1, 4)\n",
    "        anchors = anchors.view(1, K * A, 4).expand(batch_size, K * A, 4)\n",
    "\n",
    "        # Transpose and reshape predicted bbox transformations to get them\n",
    "        # into the same order as the anchors:\n",
    "\n",
    "        bbox_deltas = bbox_deltas.permute(0, 2, 3, 1).contiguous()\n",
    "        bbox_deltas = bbox_deltas.view(batch_size, -1, 4)\n",
    "\n",
    "        # Same story for the scores:\n",
    "        scores = scores.permute(0, 2, 3, 1).contiguous()\n",
    "        scores = scores.view(batch_size, -1)\n",
    "\n",
    "        # Convert anchors into proposals via bbox transformations\n",
    "        proposals = bbox_transform_inv(anchors, bbox_deltas, batch_size)\n",
    "\n",
    "        # 2. clip predicted boxes to image\n",
    "        proposals = clip_boxes(proposals, im_info, batch_size)\n",
    "        # proposals = clip_boxes_batch(proposals, im_info, batch_size)\n",
    "\n",
    "        # assign the score to 0 if it's non keep.\n",
    "        # keep = self._filter_boxes(proposals, min_size * im_info[:, 2])\n",
    "\n",
    "        # trim keep index to make it euqal over batch\n",
    "        # keep_idx = torch.cat(tuple(keep_idx), 0)\n",
    "\n",
    "        # scores_keep = scores.view(-1)[keep_idx].view(batch_size, trim_size)\n",
    "        # proposals_keep = proposals.view(-1, 4)[keep_idx, :].contiguous().view(batch_size, trim_size, 4)\n",
    "\n",
    "        # _, order = torch.sort(scores_keep, 1, True)\n",
    "\n",
    "        scores_keep = scores\n",
    "        proposals_keep = proposals\n",
    "        _, order = torch.sort(scores_keep, 1, True)\n",
    "\n",
    "        output = scores.new(batch_size, post_nms_topN, 5).zero_()\n",
    "        for i in range(batch_size):\n",
    "            # # 3. remove predicted boxes with either height or width < threshold\n",
    "            # # (NOTE: convert min_size to input image scale stored in im_info[2])\n",
    "            proposals_single = proposals_keep[i]\n",
    "            scores_single = scores_keep[i]\n",
    "\n",
    "            # # 4. sort all (proposal, score) pairs by score from highest to lowest\n",
    "            # # 5. take top pre_nms_topN (e.g. 6000)\n",
    "            order_single = order[i]\n",
    "\n",
    "            if pre_nms_topN > 0 and pre_nms_topN < scores_keep.numel():\n",
    "                order_single = order_single[:pre_nms_topN]\n",
    "\n",
    "            proposals_single = proposals_single[order_single, :]\n",
    "            scores_single = scores_single[order_single].view(-1,1)\n",
    "\n",
    "            # 6. apply nms (e.g. threshold = 0.7)\n",
    "            # 7. take after_nms_topN (e.g. 300)\n",
    "            # 8. return the top proposals (-> RoIs top)\n",
    "            keep_idx_i = nms(proposals_single, scores_single.squeeze(1), nms_thresh)\n",
    "            keep_idx_i = keep_idx_i.long().view(-1)\n",
    "\n",
    "            if post_nms_topN > 0:\n",
    "                keep_idx_i = keep_idx_i[:post_nms_topN]\n",
    "            proposals_single = proposals_single[keep_idx_i, :]\n",
    "            scores_single = scores_single[keep_idx_i, :]\n",
    "\n",
    "            # padding 0 at the end.\n",
    "            num_proposal = proposals_single.size(0)\n",
    "            output[i,:,0] = i\n",
    "            output[i,:num_proposal,1:] = proposals_single\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, top, propagate_down, bottom):\n",
    "        \"\"\"This layer does not propagate gradients.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def reshape(self, bottom, top):\n",
    "        \"\"\"Reshaping happens during the call to forward.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def _filter_boxes(self, boxes, min_size):\n",
    "        \"\"\"Remove all boxes with any side smaller than min_size.\"\"\"\n",
    "        ws = boxes[:, :, 2] - boxes[:, :, 0] + 1\n",
    "        hs = boxes[:, :, 3] - boxes[:, :, 1] + 1\n",
    "        keep = ((ws >= min_size.view(-1,1).expand_as(ws)) & (hs >= min_size.view(-1,1).expand_as(hs)))\n",
    "        return keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tB2w6v-sFqCY"
   },
   "outputs": [],
   "source": [
    "class _AnchorTargetLayer(nn.Module):\n",
    "    \"\"\"\n",
    "        Assign anchors to ground-truth targets. Produces anchor classification\n",
    "        labels and bounding-box regression targets.\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_stride, scales, ratios):\n",
    "        super(_AnchorTargetLayer, self).__init__()\n",
    "\n",
    "        self._feat_stride = feat_stride\n",
    "        self._scales = scales\n",
    "        anchor_scales = scales\n",
    "        self._anchors = torch.from_numpy(generate_anchors(scales=np.array(anchor_scales), ratios=np.array(ratios))).float()\n",
    "        self._num_anchors = self._anchors.size(0)\n",
    "\n",
    "        # allow boxes to sit over the edge by a small amount\n",
    "        self._allowed_border = 0  # default is 0\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Algorithm:\n",
    "        #\n",
    "        # for each (H, W) location i\n",
    "        #   generate 9 anchor boxes centered on cell i\n",
    "        #   apply predicted bbox deltas at cell i to each of the 9 anchors\n",
    "        # filter out-of-image anchors\n",
    "\n",
    "        rpn_cls_score = input[0]\n",
    "        gt_boxes = input[1]\n",
    "        im_info = input[2]\n",
    "        num_boxes = input[3]\n",
    "\n",
    "        # map of shape (..., H, W)\n",
    "        height, width = rpn_cls_score.size(2), rpn_cls_score.size(3)\n",
    "\n",
    "        batch_size = gt_boxes.size(0)\n",
    "\n",
    "        feat_height, feat_width = rpn_cls_score.size(2), rpn_cls_score.size(3)\n",
    "        shift_x = np.arange(0, feat_width) * self._feat_stride\n",
    "        shift_y = np.arange(0, feat_height) * self._feat_stride\n",
    "        shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "        shifts = torch.from_numpy(np.vstack((shift_x.ravel(), shift_y.ravel(),\n",
    "                                  shift_x.ravel(), shift_y.ravel())).transpose())\n",
    "        shifts = shifts.contiguous().type_as(rpn_cls_score).float()\n",
    "\n",
    "        A = self._num_anchors\n",
    "        K = shifts.size(0)\n",
    "\n",
    "        self._anchors = self._anchors.type_as(gt_boxes) # move to specific gpu.\n",
    "        all_anchors = self._anchors.view(1, A, 4) + shifts.view(K, 1, 4)\n",
    "        all_anchors = all_anchors.view(K * A, 4)\n",
    "\n",
    "        total_anchors = int(K * A)\n",
    "\n",
    "        keep = ((all_anchors[:, 0] >= -self._allowed_border) &\n",
    "                (all_anchors[:, 1] >= -self._allowed_border) &\n",
    "                (all_anchors[:, 2] < int(im_info[0][1]) + self._allowed_border) &\n",
    "                (all_anchors[:, 3] < int(im_info[0][0]) + self._allowed_border))\n",
    "\n",
    "        inds_inside = torch.nonzero(keep).view(-1)\n",
    "\n",
    "        # keep only inside anchors\n",
    "        anchors = all_anchors[inds_inside, :]\n",
    "\n",
    "        # label: 1 is positive, 0 is negative, -1 is dont care\n",
    "        labels = gt_boxes.new(batch_size, inds_inside.size(0)).fill_(-1)\n",
    "        bbox_inside_weights = gt_boxes.new(batch_size, inds_inside.size(0)).zero_()\n",
    "        bbox_outside_weights = gt_boxes.new(batch_size, inds_inside.size(0)).zero_()\n",
    "\n",
    "        overlaps = bbox_overlaps_batch(anchors, gt_boxes)\n",
    "\n",
    "        max_overlaps, argmax_overlaps = torch.max(overlaps, 2)\n",
    "        gt_max_overlaps, _ = torch.max(overlaps, 1)\n",
    "\n",
    "        if not False:\n",
    "            labels[max_overlaps < 0.3] = 0\n",
    "\n",
    "        gt_max_overlaps[gt_max_overlaps==0] = 1e-5\n",
    "        keep = torch.sum(overlaps.eq(gt_max_overlaps.view(batch_size,1,-1).expand_as(overlaps)), 2)\n",
    "\n",
    "        if torch.sum(keep) > 0:\n",
    "            labels[keep>0] = 1\n",
    "\n",
    "        # fg label: above threshold IOU\n",
    "        labels[max_overlaps >= 0.7] = 1\n",
    "\n",
    "        if False:\n",
    "            labels[max_overlaps < 0.3] = 0\n",
    "\n",
    "        num_fg = int(0.5 * 256)\n",
    "\n",
    "        sum_fg = torch.sum((labels == 1).int(), 1)\n",
    "        sum_bg = torch.sum((labels == 0).int(), 1)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # subsample positive labels if we have too many\n",
    "            if sum_fg[i] > num_fg:\n",
    "                fg_inds = torch.nonzero(labels[i] == 1).view(-1)\n",
    "                # torch.randperm seems has a bug on multi-gpu setting that cause the segfault.\n",
    "                # See https://github.com/pytorch/pytorch/issues/1868 for more details.\n",
    "                # use numpy instead.\n",
    "                #rand_num = torch.randperm(fg_inds.size(0)).type_as(gt_boxes).long()\n",
    "                rand_num = torch.from_numpy(np.random.permutation(fg_inds.size(0))).type_as(gt_boxes).long()\n",
    "                disable_inds = fg_inds[rand_num[:fg_inds.size(0)-num_fg]]\n",
    "                labels[i][disable_inds] = -1\n",
    "\n",
    "#           num_bg = cfg.TRAIN.RPN_BATCHSIZE - sum_fg[i]\n",
    "            num_bg = 256 - torch.sum((labels == 1).int(), 1)[i]\n",
    "\n",
    "            # subsample negative labels if we have too many\n",
    "            if sum_bg[i] > num_bg:\n",
    "                bg_inds = torch.nonzero(labels[i] == 0).view(-1)\n",
    "                #rand_num = torch.randperm(bg_inds.size(0)).type_as(gt_boxes).long()\n",
    "\n",
    "                rand_num = torch.from_numpy(np.random.permutation(bg_inds.size(0))).type_as(gt_boxes).long()\n",
    "                disable_inds = bg_inds[rand_num[:bg_inds.size(0)-num_bg]]\n",
    "                labels[i][disable_inds] = -1\n",
    "\n",
    "        offset = torch.arange(0, batch_size)*gt_boxes.size(1)\n",
    "\n",
    "        argmax_overlaps = argmax_overlaps + offset.view(batch_size, 1).type_as(argmax_overlaps)\n",
    "        bbox_targets = _compute_targets_batch(anchors, gt_boxes.view(-1,5)[argmax_overlaps.view(-1), :].view(batch_size, -1, 5))\n",
    "\n",
    "        # use a single value instead of 4 values for easy index.\n",
    "        bbox_inside_weights[labels==1] = 1.0\n",
    "\n",
    "        if -1.0 < 0:\n",
    "            num_examples = torch.sum(labels[i] >= 0)\n",
    "            positive_weights = 1.0 / num_examples.item()\n",
    "            negative_weights = 1.0 / num_examples.item()\n",
    "        else:\n",
    "            assert ((-1.0 > 0) &\n",
    "                    (-1.0 < 1))\n",
    "\n",
    "        bbox_outside_weights[labels == 1] = positive_weights\n",
    "        bbox_outside_weights[labels == 0] = negative_weights\n",
    "\n",
    "        labels = _unmap(labels, total_anchors, inds_inside, batch_size, fill=-1)\n",
    "        bbox_targets = _unmap(bbox_targets, total_anchors, inds_inside, batch_size, fill=0)\n",
    "        bbox_inside_weights = _unmap(bbox_inside_weights, total_anchors, inds_inside, batch_size, fill=0)\n",
    "        bbox_outside_weights = _unmap(bbox_outside_weights, total_anchors, inds_inside, batch_size, fill=0)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        labels = labels.view(batch_size, height, width, A).permute(0,3,1,2).contiguous()\n",
    "        labels = labels.view(batch_size, 1, A * height, width)\n",
    "        outputs.append(labels)\n",
    "\n",
    "        bbox_targets = bbox_targets.view(batch_size, height, width, A*4).permute(0,3,1,2).contiguous()\n",
    "        outputs.append(bbox_targets)\n",
    "\n",
    "        anchors_count = bbox_inside_weights.size(1)\n",
    "        bbox_inside_weights = bbox_inside_weights.view(batch_size,anchors_count,1).expand(batch_size, anchors_count, 4)\n",
    "\n",
    "        bbox_inside_weights = bbox_inside_weights.contiguous().view(batch_size, height, width, 4*A)\\\n",
    "                            .permute(0,3,1,2).contiguous()\n",
    "\n",
    "        outputs.append(bbox_inside_weights)\n",
    "\n",
    "        bbox_outside_weights = bbox_outside_weights.view(batch_size,anchors_count,1).expand(batch_size, anchors_count, 4)\n",
    "        bbox_outside_weights = bbox_outside_weights.contiguous().view(batch_size, height, width, 4*A)\\\n",
    "                            .permute(0,3,1,2).contiguous()\n",
    "        outputs.append(bbox_outside_weights)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, top, propagate_down, bottom):\n",
    "        \"\"\"This layer does not propagate gradients.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def reshape(self, bottom, top):\n",
    "        \"\"\"Reshaping happens during the call to forward.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M3t7aKtcrsfZ"
   },
   "outputs": [],
   "source": [
    "def _unmap(data, count, inds, batch_size, fill=0):\n",
    "    \"\"\" Unmap a subset of item (data) back to the original set of items (of\n",
    "    size count) \"\"\"\n",
    "\n",
    "    if data.dim() == 2:\n",
    "        ret = torch.Tensor(batch_size, count).fill_(fill).type_as(data)\n",
    "        ret[:, inds] = data\n",
    "    else:\n",
    "        ret = torch.Tensor(batch_size, count, data.size(2)).fill_(fill).type_as(data)\n",
    "        ret[:, inds,:] = data\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tkQB-JQ7vu47"
   },
   "outputs": [],
   "source": [
    "class _ROIPool(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, roi, output_size, spatial_scale):\n",
    "        ctx.output_size = _pair(output_size)\n",
    "        ctx.spatial_scale = spatial_scale\n",
    "        ctx.input_shape = input.size()\n",
    "        output, argmax = _C.roi_pool_forward(\n",
    "            input, roi, spatial_scale, output_size[0], output_size[1]\n",
    "        )\n",
    "        ctx.save_for_backward(input, roi, argmax)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    @once_differentiable\n",
    "    def backward(ctx, grad_output):\n",
    "        input, rois, argmax = ctx.saved_tensors\n",
    "        output_size = ctx.output_size\n",
    "        spatial_scale = ctx.spatial_scale\n",
    "        bs, ch, h, w = ctx.input_shape\n",
    "        grad_input = _C.roi_pool_backward(\n",
    "            grad_output,\n",
    "            input,\n",
    "            rois,\n",
    "            argmax,\n",
    "            spatial_scale,\n",
    "            output_size[0],\n",
    "            output_size[1],\n",
    "            bs,\n",
    "            ch,\n",
    "            h,\n",
    "            w,\n",
    "        )\n",
    "        return grad_input, None, None, None\n",
    "\n",
    "\n",
    "roi_pool = _ROIPool.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iS3mZSkgIhap"
   },
   "outputs": [],
   "source": [
    "class ROIPool(nn.Module):\n",
    "    def __init__(self, output_size, spatial_scale):\n",
    "        super(ROIPool, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.spatial_scale = spatial_scale\n",
    "\n",
    "    def forward(self, input, rois):\n",
    "        return roi_pool(input, rois, self.output_size, self.spatial_scale)\n",
    "\n",
    "    def __repr__(self):\n",
    "        tmpstr = self.__class__.__name__ + \"(\"\n",
    "        tmpstr += \"output_size=\" + str(self.output_size)\n",
    "        tmpstr += \", spatial_scale=\" + str(self.spatial_scale)\n",
    "        tmpstr += \")\"\n",
    "        return tmpstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SYtReEPEuLTR"
   },
   "outputs": [],
   "source": [
    "class _ROIAlign(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, roi, output_size, spatial_scale, sampling_ratio):\n",
    "        ctx.save_for_backward(roi)\n",
    "        ctx.output_size = _pair(output_size)\n",
    "        ctx.spatial_scale = spatial_scale\n",
    "        ctx.sampling_ratio = sampling_ratio\n",
    "        ctx.input_shape = input.size()\n",
    "        output = _C.roi_align_forward(input, roi, spatial_scale, output_size[0], output_size[1], sampling_ratio)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    @once_differentiable\n",
    "    def backward(ctx, grad_output):\n",
    "        rois, = ctx.saved_tensors\n",
    "        output_size = ctx.output_size\n",
    "        spatial_scale = ctx.spatial_scale\n",
    "        sampling_ratio = ctx.sampling_ratio\n",
    "        bs, ch, h, w = ctx.input_shape\n",
    "        grad_input = _C.roi_align_backward(\n",
    "            grad_output,\n",
    "            rois,\n",
    "            spatial_scale,\n",
    "            output_size[0],\n",
    "            output_size[1],\n",
    "            bs,\n",
    "            ch,\n",
    "            h,\n",
    "            w,\n",
    "            sampling_ratio,\n",
    "        )\n",
    "        return grad_input, None, None, None, None\n",
    "\n",
    "\n",
    "roi_align = _ROIAlign.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pF8wFR_kIlbq"
   },
   "outputs": [],
   "source": [
    "class ROIAlign(nn.Module):\n",
    "    def __init__(self, output_size, spatial_scale, sampling_ratio):\n",
    "        super(ROIAlign, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.spatial_scale = spatial_scale\n",
    "        self.sampling_ratio = sampling_ratio\n",
    "\n",
    "    def forward(self, input, rois):\n",
    "        return roi_align(\n",
    "            input, rois, self.output_size, self.spatial_scale, self.sampling_ratio\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        tmpstr = self.__class__.__name__ + \"(\"\n",
    "        tmpstr += \"output_size=\" + str(self.output_size)\n",
    "        tmpstr += \", spatial_scale=\" + str(self.spatial_scale)\n",
    "        tmpstr += \", sampling_ratio=\" + str(self.sampling_ratio)\n",
    "        tmpstr += \")\"\n",
    "        return tmpstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-hHdtXMBYazt"
   },
   "outputs": [],
   "source": [
    "@cupy.util.memoize(for_each_device=True)\n",
    "def load_kernel(kernel_name, code, **kwargs):\n",
    "    cp.cuda.runtime.free(0)\n",
    "    code = Template(code).substitute(**kwargs)\n",
    "    kernel_code = cupy.cuda.compile_with_cache(code)\n",
    "    return kernel_code.get_function(kernel_name)\n",
    "\n",
    "\n",
    "CUDA_NUM_THREADS = 1024\n",
    "\n",
    "Stream = namedtuple('Stream', ['ptr'])\n",
    "\n",
    "def GET_BLOCKS(N, K=CUDA_NUM_THREADS):\n",
    "    return (N + K - 1) // K\n",
    "\n",
    "class RoI(Function):\n",
    "    \"\"\"\n",
    "    NOTE：only CUDA-compatible\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, outh, outw, spatial_scale):\n",
    "        self.forward_fn = load_kernel('roi_forward', kernel_forward)\n",
    "        self.backward_fn = load_kernel('roi_backward', kernel_backward)\n",
    "        self.outh, self.outw, self.spatial_scale = outh, outw, spatial_scale\n",
    "\n",
    "    def forward(self, x, rois):\n",
    "        # NOTE: MAKE SURE input is contiguous too\n",
    "        x = x.contiguous()\n",
    "        rois = rois.contiguous()\n",
    "        self.in_size = B, C, H, W = x.size()\n",
    "        self.N = N = rois.size(0)\n",
    "        output = torch.zeros(N, C, self.outh, self.outw).cuda()\n",
    "        self.argmax_data = torch.zeros(N, C, self.outh, self.outw).int().cuda()\n",
    "        self.rois = rois\n",
    "        args = [x.data_ptr(), rois.data_ptr(),\n",
    "                output.data_ptr(),\n",
    "                self.argmax_data.data_ptr(),\n",
    "                self.spatial_scale, C, H, W,\n",
    "                self.outh, self.outw,\n",
    "                output.numel()]\n",
    "        stream = Stream(ptr=torch.cuda.current_stream().cuda_stream)\n",
    "        self.forward_fn(args=args,\n",
    "                        block=(CUDA_NUM_THREADS, 1, 1),\n",
    "                        grid=(GET_BLOCKS(output.numel()), 1, 1),\n",
    "                        stream=stream)\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        ##NOTE: IMPORTANT CONTIGUOUS\n",
    "        # TODO: input\n",
    "        grad_output = grad_output.contiguous()\n",
    "        B, C, H, W = self.in_size\n",
    "        grad_input = torch.zeros(self.in_size).cuda()\n",
    "        stream = Stream(ptr=torch.cuda.current_stream().cuda_stream)\n",
    "        args = [grad_output.data_ptr(),\n",
    "                self.argmax_data.data_ptr(),\n",
    "                self.rois.data_ptr(),\n",
    "                grad_input.data_ptr(),\n",
    "                self.N, self.spatial_scale, C, H, W, self.outh, self.outw,\n",
    "                grad_input.numel()]\n",
    "        self.backward_fn(args=args,\n",
    "                         block=(CUDA_NUM_THREADS, 1, 1),\n",
    "                         grid=(GET_BLOCKS(grad_input.numel()), 1, 1),\n",
    "                         stream=stream\n",
    "                         )\n",
    "        return grad_input, None\n",
    "\n",
    "\n",
    "class RoIPooling2D(nn.Module):\n",
    "\n",
    "    def __init__(self, outh, outw, spatial_scale):\n",
    "        super(RoIPooling2D, self).__init__()\n",
    "        self.RoI = RoI(outh, outw, spatial_scale)\n",
    "\n",
    "    def forward(self, x, rois):\n",
    "        return self.RoI(x, rois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BM-LjT_fY7Xn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "beKOSwP3rYoY"
   },
   "outputs": [],
   "source": [
    "def bbox_transform_batch(ex_rois, gt_rois):\n",
    "\n",
    "    if ex_rois.dim() == 2:\n",
    "        ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\n",
    "        ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\n",
    "        ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n",
    "        ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n",
    "\n",
    "        gt_widths = gt_rois[:, :, 2] - gt_rois[:, :, 0] + 1.0\n",
    "        gt_heights = gt_rois[:, :, 3] - gt_rois[:, :, 1] + 1.0\n",
    "        gt_ctr_x = gt_rois[:, :, 0] + 0.5 * gt_widths\n",
    "        gt_ctr_y = gt_rois[:, :, 1] + 0.5 * gt_heights\n",
    "\n",
    "        targets_dx = (gt_ctr_x - ex_ctr_x.view(1,-1).expand_as(gt_ctr_x)) / ex_widths\n",
    "        targets_dy = (gt_ctr_y - ex_ctr_y.view(1,-1).expand_as(gt_ctr_y)) / ex_heights\n",
    "        targets_dw = torch.log(gt_widths / ex_widths.view(1,-1).expand_as(gt_widths))\n",
    "        targets_dh = torch.log(gt_heights / ex_heights.view(1,-1).expand_as(gt_heights))\n",
    "\n",
    "    elif ex_rois.dim() == 3:\n",
    "        ex_widths = ex_rois[:, :, 2] - ex_rois[:, :, 0] + 1.0\n",
    "        ex_heights = ex_rois[:,:, 3] - ex_rois[:,:, 1] + 1.0\n",
    "        ex_ctr_x = ex_rois[:, :, 0] + 0.5 * ex_widths\n",
    "        ex_ctr_y = ex_rois[:, :, 1] + 0.5 * ex_heights\n",
    "\n",
    "        gt_widths = gt_rois[:, :, 2] - gt_rois[:, :, 0] + 1.0\n",
    "        gt_heights = gt_rois[:, :, 3] - gt_rois[:, :, 1] + 1.0\n",
    "        gt_ctr_x = gt_rois[:, :, 0] + 0.5 * gt_widths\n",
    "        gt_ctr_y = gt_rois[:, :, 1] + 0.5 * gt_heights\n",
    "\n",
    "        targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n",
    "        targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n",
    "        targets_dw = torch.log(gt_widths / ex_widths)\n",
    "        targets_dh = torch.log(gt_heights / ex_heights)\n",
    "    else:\n",
    "        raise ValueError('ex_roi input dimension is not correct.')\n",
    "\n",
    "    targets = torch.stack(\n",
    "        (targets_dx, targets_dy, targets_dw, targets_dh),2)\n",
    "\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sDyQ2vgMq9nS"
   },
   "outputs": [],
   "source": [
    "def bbox_overlaps_batch(anchors, gt_boxes):\n",
    "    \"\"\"\n",
    "    anchors: (N, 4) ndarray of float\n",
    "    gt_boxes: (b, K, 5) ndarray of float\n",
    "\n",
    "    overlaps: (N, K) ndarray of overlap between boxes and query_boxes\n",
    "    \"\"\"\n",
    "    batch_size = gt_boxes.size(0)\n",
    "\n",
    "\n",
    "    if anchors.dim() == 2:\n",
    "\n",
    "        N = anchors.size(0)\n",
    "        K = gt_boxes.size(1)\n",
    "\n",
    "        anchors = anchors.view(1, N, 4).expand(batch_size, N, 4).contiguous()\n",
    "        gt_boxes = gt_boxes[:,:,:4].contiguous()\n",
    "\n",
    "\n",
    "        gt_boxes_x = (gt_boxes[:,:,2] - gt_boxes[:,:,0] + 1)\n",
    "        gt_boxes_y = (gt_boxes[:,:,3] - gt_boxes[:,:,1] + 1)\n",
    "        gt_boxes_area = (gt_boxes_x * gt_boxes_y).view(batch_size, 1, K)\n",
    "\n",
    "        anchors_boxes_x = (anchors[:,:,2] - anchors[:,:,0] + 1)\n",
    "        anchors_boxes_y = (anchors[:,:,3] - anchors[:,:,1] + 1)\n",
    "        anchors_area = (anchors_boxes_x * anchors_boxes_y).view(batch_size, N, 1)\n",
    "\n",
    "        gt_area_zero = (gt_boxes_x == 1) & (gt_boxes_y == 1)\n",
    "        anchors_area_zero = (anchors_boxes_x == 1) & (anchors_boxes_y == 1)\n",
    "\n",
    "        boxes = anchors.view(batch_size, N, 1, 4).expand(batch_size, N, K, 4)\n",
    "        query_boxes = gt_boxes.view(batch_size, 1, K, 4).expand(batch_size, N, K, 4)\n",
    "\n",
    "        iw = (torch.min(boxes[:,:,:,2], query_boxes[:,:,:,2]) -\n",
    "            torch.max(boxes[:,:,:,0], query_boxes[:,:,:,0]) + 1)\n",
    "        iw[iw < 0] = 0\n",
    "\n",
    "        ih = (torch.min(boxes[:,:,:,3], query_boxes[:,:,:,3]) -\n",
    "            torch.max(boxes[:,:,:,1], query_boxes[:,:,:,1]) + 1)\n",
    "        ih[ih < 0] = 0\n",
    "        ua = anchors_area + gt_boxes_area - (iw * ih)\n",
    "        overlaps = iw * ih / ua\n",
    "\n",
    "        # mask the overlap here.\n",
    "        overlaps.masked_fill_(gt_area_zero.view(batch_size, 1, K).expand(batch_size, N, K), 0)\n",
    "        overlaps.masked_fill_(anchors_area_zero.view(batch_size, N, 1).expand(batch_size, N, K), -1)\n",
    "\n",
    "    elif anchors.dim() == 3:\n",
    "        N = anchors.size(1)\n",
    "        K = gt_boxes.size(1)\n",
    "\n",
    "        if anchors.size(2) == 4:\n",
    "            anchors = anchors[:,:,:4].contiguous()\n",
    "        else:\n",
    "            anchors = anchors[:,:,1:5].contiguous()\n",
    "\n",
    "        gt_boxes = gt_boxes[:,:,:4].contiguous()\n",
    "\n",
    "        gt_boxes_x = (gt_boxes[:,:,2] - gt_boxes[:,:,0] + 1)\n",
    "        gt_boxes_y = (gt_boxes[:,:,3] - gt_boxes[:,:,1] + 1)\n",
    "        gt_boxes_area = (gt_boxes_x * gt_boxes_y).view(batch_size, 1, K)\n",
    "\n",
    "        anchors_boxes_x = (anchors[:,:,2] - anchors[:,:,0] + 1)\n",
    "        anchors_boxes_y = (anchors[:,:,3] - anchors[:,:,1] + 1)\n",
    "        anchors_area = (anchors_boxes_x * anchors_boxes_y).view(batch_size, N, 1)\n",
    "\n",
    "        gt_area_zero = (gt_boxes_x == 1) & (gt_boxes_y == 1)\n",
    "        anchors_area_zero = (anchors_boxes_x == 1) & (anchors_boxes_y == 1)\n",
    "\n",
    "        boxes = anchors.view(batch_size, N, 1, 4).expand(batch_size, N, K, 4)\n",
    "        query_boxes = gt_boxes.view(batch_size, 1, K, 4).expand(batch_size, N, K, 4)\n",
    "\n",
    "        iw = (torch.min(boxes[:,:,:,2], query_boxes[:,:,:,2]) -\n",
    "            torch.max(boxes[:,:,:,0], query_boxes[:,:,:,0]) + 1)\n",
    "        iw[iw < 0] = 0\n",
    "\n",
    "        ih = (torch.min(boxes[:,:,:,3], query_boxes[:,:,:,3]) -\n",
    "            torch.max(boxes[:,:,:,1], query_boxes[:,:,:,1]) + 1)\n",
    "        ih[ih < 0] = 0\n",
    "        ua = anchors_area + gt_boxes_area - (iw * ih)\n",
    "\n",
    "        overlaps = iw * ih / ua\n",
    "\n",
    "        # mask the overlap here.\n",
    "        overlaps.masked_fill_(gt_area_zero.view(batch_size, 1, K).expand(batch_size, N, K), 0)\n",
    "        overlaps.masked_fill_(anchors_area_zero.view(batch_size, N, 1).expand(batch_size, N, K), -1)\n",
    "    else:\n",
    "        raise ValueError('anchors input dimension is not correct.')\n",
    "\n",
    "    return overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_vXTQvwbsII0"
   },
   "outputs": [],
   "source": [
    "def _smooth_l1_loss(bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights, sigma=1.0, dim=[1]):\n",
    "\n",
    "    sigma_2 = sigma ** 2\n",
    "    box_diff = bbox_pred - bbox_targets\n",
    "    in_box_diff = bbox_inside_weights * box_diff\n",
    "    abs_in_box_diff = torch.abs(in_box_diff)\n",
    "    smoothL1_sign = (abs_in_box_diff < 1. / sigma_2).detach().float()\n",
    "    in_loss_box = torch.pow(in_box_diff, 2) * (sigma_2 / 2.) * smoothL1_sign \\\n",
    "                  + (abs_in_box_diff - (0.5 / sigma_2)) * (1. - smoothL1_sign)\n",
    "    out_loss_box = bbox_outside_weights * in_loss_box\n",
    "    loss_box = out_loss_box\n",
    "    for i in sorted(dim, reverse=True):\n",
    "        loss_box = loss_box.sum(i)\n",
    "    loss_box = loss_box.mean()\n",
    "    return loss_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "THQAWrcEHYAD"
   },
   "outputs": [],
   "source": [
    "class _ProposalTargetLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Assign object detection proposals to ground-truth targets. Produces proposal\n",
    "    classification labels and bounding-box regression targets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nclasses):\n",
    "        super(_ProposalTargetLayer, self).__init__()\n",
    "        self._num_classes = nclasses\n",
    "        self.BBOX_NORMALIZE_MEANS = torch.FloatTensor([0.0, 0.0, 0.0, 0.0])\n",
    "        self.BBOX_NORMALIZE_STDS = torch.FloatTensor([0.1, 0.1, 0.2, 0.2])\n",
    "        self.BBOX_INSIDE_WEIGHTS = torch.FloatTensor([1.0, 1.0, 1.0, 1.0])\n",
    "\n",
    "    def forward(self, all_rois, gt_boxes, num_boxes):\n",
    "\n",
    "        self.BBOX_NORMALIZE_MEANS = self.BBOX_NORMALIZE_MEANS.type_as(gt_boxes)\n",
    "        self.BBOX_NORMALIZE_STDS = self.BBOX_NORMALIZE_STDS.type_as(gt_boxes)\n",
    "        self.BBOX_INSIDE_WEIGHTS = self.BBOX_INSIDE_WEIGHTS.type_as(gt_boxes)\n",
    "\n",
    "        gt_boxes_append = gt_boxes.new(gt_boxes.size()).zero_()\n",
    "        gt_boxes_append[:,:,1:5] = gt_boxes[:,:,:4]\n",
    "\n",
    "        # Include ground-truth boxes in the set of candidate rois\n",
    "        all_rois = torch.cat([all_rois, gt_boxes_append], 1)\n",
    "\n",
    "        num_images = 1\n",
    "        rois_per_image = int(256 / num_images)\n",
    "        fg_rois_per_image = int(np.round(0.25 * rois_per_image))\n",
    "        fg_rois_per_image = 1 if fg_rois_per_image == 0 else fg_rois_per_image\n",
    "\n",
    "        labels, rois, bbox_targets, bbox_inside_weights = self._sample_rois_pytorch(\n",
    "            all_rois, gt_boxes, fg_rois_per_image,\n",
    "            rois_per_image, self._num_classes)\n",
    "\n",
    "        bbox_outside_weights = (bbox_inside_weights > 0).float()\n",
    "\n",
    "        return rois, labels, bbox_targets, bbox_inside_weights, bbox_outside_weights\n",
    "\n",
    "    def backward(self, top, propagate_down, bottom):\n",
    "        \"\"\"This layer does not propagate gradients.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def reshape(self, bottom, top):\n",
    "        \"\"\"Reshaping happens during the call to forward.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def _get_bbox_regression_labels_pytorch(self, bbox_target_data, labels_batch, num_classes):\n",
    "        \"\"\"Bounding-box regression targets (bbox_target_data) are stored in a\n",
    "        compact form b x N x (class, tx, ty, tw, th)\n",
    "\n",
    "        This function expands those targets into the 4-of-4*K representation used\n",
    "        by the network (i.e. only one class has non-zero targets).\n",
    "\n",
    "        Returns:\n",
    "            bbox_target (ndarray): b x N x 4K blob of regression targets\n",
    "            bbox_inside_weights (ndarray): b x N x 4K blob of loss weights\n",
    "        \"\"\"\n",
    "        batch_size = labels_batch.size(0)\n",
    "        rois_per_image = labels_batch.size(1)\n",
    "        clss = labels_batch\n",
    "        bbox_targets = bbox_target_data.new(batch_size, rois_per_image, 4).zero_()\n",
    "        bbox_inside_weights = bbox_target_data.new(bbox_targets.size()).zero_()\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            # assert clss[b].sum() > 0\n",
    "            if clss[b].sum() == 0:\n",
    "                continue\n",
    "            inds = torch.nonzero(clss[b] > 0).view(-1)\n",
    "            for i in range(inds.numel()):\n",
    "                ind = inds[i]\n",
    "                bbox_targets[b, ind, :] = bbox_target_data[b, ind, :]\n",
    "                bbox_inside_weights[b, ind, :] = self.BBOX_INSIDE_WEIGHTS\n",
    "\n",
    "        return bbox_targets, bbox_inside_weights\n",
    "\n",
    "\n",
    "    def _compute_targets_pytorch(self, ex_rois, gt_rois):\n",
    "        \"\"\"Compute bounding-box regression targets for an image.\"\"\"\n",
    "\n",
    "        assert ex_rois.size(1) == gt_rois.size(1)\n",
    "        assert ex_rois.size(2) == 4\n",
    "        assert gt_rois.size(2) == 4\n",
    "\n",
    "        batch_size = ex_rois.size(0)\n",
    "        rois_per_image = ex_rois.size(1)\n",
    "\n",
    "        targets = bbox_transform_batch(ex_rois, gt_rois)\n",
    "\n",
    "        if True:\n",
    "            # Optionally normalize targets by a precomputed mean and stdev\n",
    "            targets = ((targets - self.BBOX_NORMALIZE_MEANS.expand_as(targets))\n",
    "                        / self.BBOX_NORMALIZE_STDS.expand_as(targets))\n",
    "\n",
    "        return targets\n",
    "\n",
    "\n",
    "    def _sample_rois_pytorch(self, all_rois, gt_boxes, fg_rois_per_image, rois_per_image, num_classes):\n",
    "        \"\"\"Generate a random sample of RoIs comprising foreground and background\n",
    "        examples.\n",
    "        \"\"\"\n",
    "        # overlaps: (rois x gt_boxes)\n",
    "\n",
    "        overlaps = bbox_overlaps_batch(all_rois, gt_boxes)\n",
    "\n",
    "        max_overlaps, gt_assignment = torch.max(overlaps, 2)\n",
    "\n",
    "        batch_size = overlaps.size(0)\n",
    "        num_proposal = overlaps.size(1)\n",
    "        num_boxes_per_img = overlaps.size(2)\n",
    "\n",
    "        offset = torch.arange(0, batch_size)*gt_boxes.size(1)\n",
    "        offset = offset.view(-1, 1).type_as(gt_assignment) + gt_assignment\n",
    "\n",
    "        # changed indexing way for pytorch 1.0\n",
    "        labels = gt_boxes[:,:,4].contiguous().view(-1)[(offset.view(-1),)].view(batch_size, -1)\n",
    "\n",
    "        labels_batch = labels.new(batch_size, rois_per_image).zero_()\n",
    "        rois_batch  = all_rois.new(batch_size, rois_per_image, 5).zero_()\n",
    "        gt_rois_batch = all_rois.new(batch_size, rois_per_image, 5).zero_()\n",
    "        # Guard against the case when an image has fewer than max_fg_rois_per_image\n",
    "        # foreground RoIs\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            fg_inds = torch.nonzero(max_overlaps[i] >= 0.5).view(-1)\n",
    "            fg_num_rois = fg_inds.numel()\n",
    "\n",
    "            # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n",
    "            bg_inds = torch.nonzero((max_overlaps[i] < 0.5) &\n",
    "                                    (max_overlaps[i] >= 0.0)).view(-1)\n",
    "            bg_num_rois = bg_inds.numel()\n",
    "\n",
    "            if fg_num_rois > 0 and bg_num_rois > 0:\n",
    "                # sampling fg\n",
    "                fg_rois_per_this_image = min(fg_rois_per_image, fg_num_rois)\n",
    "\n",
    "                # torch.randperm seems has a bug on multi-gpu setting that cause the segfault.\n",
    "                # See https://github.com/pytorch/pytorch/issues/1868 for more details.\n",
    "                # use numpy instead.\n",
    "                #rand_num = torch.randperm(fg_num_rois).long().cuda()\n",
    "                rand_num = torch.from_numpy(np.random.permutation(fg_num_rois)).type_as(gt_boxes).long()\n",
    "                fg_inds = fg_inds[rand_num[:fg_rois_per_this_image]]\n",
    "\n",
    "                # sampling bg\n",
    "                bg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\n",
    "\n",
    "                # Seems torch.rand has a bug, it will generate very large number and make an error.\n",
    "                # We use numpy rand instead.\n",
    "                #rand_num = (torch.rand(bg_rois_per_this_image) * bg_num_rois).long().cuda()\n",
    "                rand_num = np.floor(np.random.rand(bg_rois_per_this_image) * bg_num_rois)\n",
    "                rand_num = torch.from_numpy(rand_num).type_as(gt_boxes).long()\n",
    "                bg_inds = bg_inds[rand_num]\n",
    "\n",
    "            elif fg_num_rois > 0 and bg_num_rois == 0:\n",
    "                # sampling fg\n",
    "                #rand_num = torch.floor(torch.rand(rois_per_image) * fg_num_rois).long().cuda()\n",
    "                rand_num = np.floor(np.random.rand(rois_per_image) * fg_num_rois)\n",
    "                rand_num = torch.from_numpy(rand_num).type_as(gt_boxes).long()\n",
    "                fg_inds = fg_inds[rand_num]\n",
    "                fg_rois_per_this_image = rois_per_image\n",
    "                bg_rois_per_this_image = 0\n",
    "            elif bg_num_rois > 0 and fg_num_rois == 0:\n",
    "                # sampling bg\n",
    "                #rand_num = torch.floor(torch.rand(rois_per_image) * bg_num_rois).long().cuda()\n",
    "                rand_num = np.floor(np.random.rand(rois_per_image) * bg_num_rois)\n",
    "                rand_num = torch.from_numpy(rand_num).type_as(gt_boxes).long()\n",
    "\n",
    "                bg_inds = bg_inds[rand_num]\n",
    "                bg_rois_per_this_image = rois_per_image\n",
    "                fg_rois_per_this_image = 0\n",
    "            else:\n",
    "                raise ValueError(\"bg_num_rois = 0 and fg_num_rois = 0, this should not happen!\")\n",
    "\n",
    "            # The indices that we're selecting (both fg and bg)\n",
    "            keep_inds = torch.cat([fg_inds, bg_inds], 0)\n",
    "\n",
    "            # Select sampled values from various arrays:\n",
    "            labels_batch[i].copy_(labels[i][keep_inds])\n",
    "\n",
    "            # Clamp labels for the background RoIs to 0\n",
    "            if fg_rois_per_this_image < rois_per_image:\n",
    "                labels_batch[i][fg_rois_per_this_image:] = 0\n",
    "\n",
    "            rois_batch[i] = all_rois[i][keep_inds]\n",
    "            rois_batch[i,:,0] = i\n",
    "\n",
    "            gt_rois_batch[i] = gt_boxes[i][gt_assignment[i][keep_inds]]\n",
    "\n",
    "        bbox_target_data = self._compute_targets_pytorch(\n",
    "                rois_batch[:,:,1:5], gt_rois_batch[:,:,:4])\n",
    "\n",
    "        bbox_targets, bbox_inside_weights = \\\n",
    "                self._get_bbox_regression_labels_pytorch(bbox_target_data, labels_batch, num_classes)\n",
    "\n",
    "        return labels_batch, rois_batch, bbox_targets, bbox_inside_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O6r_XTSNPS9h"
   },
   "outputs": [],
   "source": [
    "def bbox_transform_inv(boxes, deltas, batch_size):\n",
    "    widths = boxes[:, :, 2] - boxes[:, :, 0] + 1.0\n",
    "    heights = boxes[:, :, 3] - boxes[:, :, 1] + 1.0\n",
    "    ctr_x = boxes[:, :, 0] + 0.5 * widths\n",
    "    ctr_y = boxes[:, :, 1] + 0.5 * heights\n",
    "\n",
    "    dx = deltas[:, :, 0::4]\n",
    "    dy = deltas[:, :, 1::4]\n",
    "    dw = deltas[:, :, 2::4]\n",
    "    dh = deltas[:, :, 3::4]\n",
    "\n",
    "    pred_ctr_x = dx * widths.unsqueeze(2) + ctr_x.unsqueeze(2)\n",
    "    pred_ctr_y = dy * heights.unsqueeze(2) + ctr_y.unsqueeze(2)\n",
    "    pred_w = torch.exp(dw) * widths.unsqueeze(2)\n",
    "    pred_h = torch.exp(dh) * heights.unsqueeze(2)\n",
    "\n",
    "    pred_boxes = deltas.clone()\n",
    "    # x1\n",
    "    pred_boxes[:, :, 0::4] = pred_ctr_x - 0.5 * pred_w\n",
    "    # y1\n",
    "    pred_boxes[:, :, 1::4] = pred_ctr_y - 0.5 * pred_h\n",
    "    # x2\n",
    "    pred_boxes[:, :, 2::4] = pred_ctr_x + 0.5 * pred_w\n",
    "    # y2\n",
    "    pred_boxes[:, :, 3::4] = pred_ctr_y + 0.5 * pred_h\n",
    "\n",
    "    return pred_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UH2zhv64Pb5I"
   },
   "outputs": [],
   "source": [
    "def clip_boxes(boxes, im_shape, batch_size):\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        boxes[i,:,0::4].clamp_(0, im_shape[i, 1]-1)\n",
    "        boxes[i,:,1::4].clamp_(0, im_shape[i, 0]-1)\n",
    "        boxes[i,:,2::4].clamp_(0, im_shape[i, 1]-1)\n",
    "        boxes[i,:,3::4].clamp_(0, im_shape[i, 0]-1)\n",
    "\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V3N9aWVLrOf5"
   },
   "outputs": [],
   "source": [
    "def _compute_targets_batch(ex_rois, gt_rois):\n",
    "    \"\"\"Compute bounding-box regression targets for an image.\"\"\"\n",
    "\n",
    "    return bbox_transform_batch(ex_rois, gt_rois[:, :, :4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wUtQWuYL9u8N"
   },
   "outputs": [],
   "source": [
    "class _fasterRCNN(nn.Module):\n",
    "    \"\"\" faster RCNN \"\"\"\n",
    "    def __init__(self, classes, class_agnostic):\n",
    "        super(_fasterRCNN, self).__init__()\n",
    "        self.classes = classes\n",
    "        self.n_classes = len(classes)\n",
    "        self.class_agnostic = class_agnostic\n",
    "        # loss\n",
    "        self.RCNN_loss_cls = 0\n",
    "        self.RCNN_loss_bbox = 0\n",
    "\n",
    "        # define rpn\n",
    "        self.RCNN_rpn = _RPN(self.dout_base_model)\n",
    "        self.RCNN_proposal_target = _ProposalTargetLayer(self.n_classes)\n",
    "\n",
    "        # self.RCNN_roi_pool = _RoIPooling(cfg.POOLING_SIZE, cfg.POOLING_SIZE, 1.0/16.0)\n",
    "        # self.RCNN_roi_align = RoIAlignAvg(cfg.POOLING_SIZE, cfg.POOLING_SIZE, 1.0/16.0)\n",
    "\n",
    "        self.RCNN_roi_pool = ROIPool((7, 7), 1.0/16.0)\n",
    "        self.RCNN_roi_align = ROIAlign((7, 7), 1.0/16.0, 0)\n",
    "        self.RCNN_roi = RoIPooling2D(7, 7, 1.0/16.0)\n",
    "\n",
    "    def forward(self, im_data, im_info, gt_boxes, num_boxes):\n",
    "        batch_size = im_data.size(0)\n",
    "\n",
    "        im_info = im_info.data\n",
    "        gt_boxes = gt_boxes.data\n",
    "        num_boxes = num_boxes.data\n",
    "\n",
    "        # feed image data to base model to obtain base feature map\n",
    "        base_feat = self.RCNN_base(im_data)\n",
    "\n",
    "        # feed base feature map tp RPN to obtain rois\n",
    "        rois, rpn_loss_cls, rpn_loss_bbox = self.RCNN_rpn(base_feat, im_info, gt_boxes, num_boxes)\n",
    "\n",
    "        # if it is training phrase, then use ground trubut bboxes for refining\n",
    "        if self.training:\n",
    "            roi_data = self.RCNN_proposal_target(rois, gt_boxes, num_boxes)\n",
    "            rois, rois_label, rois_target, rois_inside_ws, rois_outside_ws = roi_data\n",
    "\n",
    "            rois_label = Variable(rois_label.view(-1).long())\n",
    "            rois_target = Variable(rois_target.view(-1, rois_target.size(2)))\n",
    "            rois_inside_ws = Variable(rois_inside_ws.view(-1, rois_inside_ws.size(2)))\n",
    "            rois_outside_ws = Variable(rois_outside_ws.view(-1, rois_outside_ws.size(2)))\n",
    "        else:\n",
    "            rois_label = None\n",
    "            rois_target = None\n",
    "            rois_inside_ws = None\n",
    "            rois_outside_ws = None\n",
    "            rpn_loss_cls = 0\n",
    "            rpn_loss_bbox = 0\n",
    "\n",
    "        rois = Variable(rois)\n",
    "        # do roi pooling based on predicted rois\n",
    "\n",
    "        POOLING_MODE = 'roi'\n",
    "        if POOLING_MODE == 'align':\n",
    "            pooled_feat = self.RCNN_roi_align(base_feat, rois.view(-1, 5))\n",
    "        elif POOLING_MODE == 'pool':\n",
    "            pooled_feat = self.RCNN_roi_pool(base_feat, rois.view(-1,5))\n",
    "        elif POOLING_MODE == 'roi':\n",
    "            pooled_feat = self.RCNN_roi(base_feat, rois.view(-1,5))\n",
    "\n",
    "        # feed pooled features to top model\n",
    "        pooled_feat = self._head_to_tail(pooled_feat)\n",
    "\n",
    "        # compute bbox offset\n",
    "        bbox_pred = self.RCNN_bbox_pred(pooled_feat)\n",
    "        if self.training and not self.class_agnostic:\n",
    "            # select the corresponding columns according to roi labels\n",
    "            bbox_pred_view = bbox_pred.view(bbox_pred.size(0), int(bbox_pred.size(1) / 4), 4)\n",
    "            bbox_pred_select = torch.gather(bbox_pred_view, 1, rois_label.view(rois_label.size(0), 1, 1).expand(rois_label.size(0), 1, 4))\n",
    "            bbox_pred = bbox_pred_select.squeeze(1)\n",
    "\n",
    "        # compute object classification probability\n",
    "        cls_score = self.RCNN_cls_score(pooled_feat)\n",
    "        cls_prob = F.softmax(cls_score, 1)\n",
    "\n",
    "        RCNN_loss_cls = 0\n",
    "        RCNN_loss_bbox = 0\n",
    "\n",
    "        if self.training:\n",
    "            # classification loss\n",
    "            RCNN_loss_cls = F.cross_entropy(cls_score, rois_label)\n",
    "\n",
    "            # bounding box regression L1 loss\n",
    "            RCNN_loss_bbox = _smooth_l1_loss(bbox_pred, rois_target, rois_inside_ws, rois_outside_ws)\n",
    "\n",
    "\n",
    "        cls_prob = cls_prob.view(batch_size, rois.size(1), -1)\n",
    "        bbox_pred = bbox_pred.view(batch_size, rois.size(1), -1)\n",
    "\n",
    "        return rois, cls_prob, bbox_pred, rpn_loss_cls, rpn_loss_bbox, RCNN_loss_cls, RCNN_loss_bbox, rois_label\n",
    "\n",
    "    def _init_weights(self):\n",
    "        def normal_init(m, mean, stddev, truncated=False):\n",
    "            \"\"\"\n",
    "            weight initalizer: truncated normal and random normal.\n",
    "            \"\"\"\n",
    "            # x is a parameter\n",
    "            if truncated:\n",
    "                m.weight.data.normal_().fmod_(2).mul_(stddev).add_(mean) # not a perfect approximation\n",
    "            else:\n",
    "                m.weight.data.normal_(mean, stddev)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "        normal_init(self.RCNN_rpn.RPN_Conv, 0, 0.01, False)\n",
    "        normal_init(self.RCNN_rpn.RPN_cls_score, 0, 0.01, False)\n",
    "        normal_init(self.RCNN_rpn.RPN_bbox_pred, 0, 0.01, False)\n",
    "        normal_init(self.RCNN_cls_score, 0, 0.01, False)\n",
    "        normal_init(self.RCNN_bbox_pred, 0, 0.001, False)\n",
    "\n",
    "    def create_architecture(self):\n",
    "        self._init_modules()\n",
    "        self._init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e2UByPrK99Uc"
   },
   "outputs": [],
   "source": [
    "class vgg16(_fasterRCNN):\n",
    "    def __init__(self, classes, pretrained=False, class_agnostic=False):\n",
    "        self.model_path = 'data/pretrained_model/vgg16_caffe.pth'\n",
    "        self.dout_base_model = 512\n",
    "        self.pretrained = pretrained\n",
    "        self.class_agnostic = class_agnostic\n",
    "\n",
    "        _fasterRCNN.__init__(self, classes, class_agnostic)\n",
    "\n",
    "    def _init_modules(self):\n",
    "        vgg = models.vgg16()\n",
    "        if self.pretrained:\n",
    "            print(\"Loading pretrained weights from %s\" %(self.model_path))\n",
    "            state_dict = torch.load(self.model_path)\n",
    "            vgg.load_state_dict({k:v for k,v in state_dict.items() if k in vgg.state_dict()})\n",
    "\n",
    "        vgg.classifier = nn.Sequential(*list(vgg.classifier._modules.values())[:-1])\n",
    "\n",
    "        # not using the last maxpool layer\n",
    "        self.RCNN_base = nn.Sequential(*list(vgg.features._modules.values())[:-1])\n",
    "\n",
    "        # Fix the layers before conv3:\n",
    "        for layer in range(10):\n",
    "            for p in self.RCNN_base[layer].parameters(): p.requires_grad = False\n",
    "\n",
    "        # self.RCNN_base = _RCNN_base(vgg.features, self.classes, self.dout_base_model)\n",
    "\n",
    "        self.RCNN_top = vgg.classifier\n",
    "\n",
    "        # not using the last maxpool layer\n",
    "        self.RCNN_cls_score = nn.Linear(4096, self.n_classes)\n",
    "\n",
    "        if self.class_agnostic:\n",
    "            self.RCNN_bbox_pred = nn.Linear(4096, 4)\n",
    "        else:\n",
    "            self.RCNN_bbox_pred = nn.Linear(4096, 4 * self.n_classes)      \n",
    "\n",
    "    def _head_to_tail(self, pool5):\n",
    "\n",
    "        pool5_flat = pool5.view(pool5.size(0), -1)\n",
    "        fc7 = self.RCNN_top(pool5_flat)\n",
    "\n",
    "        return fc7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h4XZ7Pis-hA8"
   },
   "outputs": [],
   "source": [
    "classes = ('__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3JAK7M0i-P6E"
   },
   "outputs": [],
   "source": [
    "fasterRCNN = vgg16(classes, pretrained=False, class_agnostic=False)\n",
    "fasterRCNN.create_architecture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L6_bBCkkI3mI"
   },
   "outputs": [],
   "source": [
    "fasterRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LvWIwRiqI6Ph"
   },
   "outputs": [],
   "source": [
    "im_data = np.load('im_data.npy')\n",
    "im_info = np.load('im_info.npy')\n",
    "num_boxes = np.load('num_boxes.npy')\n",
    "gt_boxes = np.load('gt_boxes.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "Liy3nReHc0A5",
    "outputId": "35c36d02-34d5-4ee6-bc96-cdc0cd31ff18"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQ03RODLdOy5"
   },
   "outputs": [],
   "source": [
    "im_data = torch.from_numpy(im_data).to(DEVICE)\n",
    "im_info = torch.from_numpy(im_info).to(DEVICE)\n",
    "num_boxes = torch.from_numpy(num_boxes).to(DEVICE)\n",
    "gt_boxes = torch.from_numpy(gt_boxes).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HozZ3kGHdw7y"
   },
   "outputs": [],
   "source": [
    "fasterRCNN = fasterRCNN.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jrDMWqTcI9xi"
   },
   "outputs": [],
   "source": [
    "rois, cls_prob, bbox_pred, rpn_loss_cls, rpn_loss_bbox, RCNN_loss_cls, RCNN_loss_bbox, rois_label = fasterRCNN(im_data, im_info, gt_boxes, num_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "Bn2jDE6dhXrl",
    "outputId": "a812fefa-baec-4eb5-dc87-086e31f36758"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.0000, 232.0871, 175.0746, 541.4301, 429.0724],\n",
       "         [  0.0000, 245.8366, 159.9640, 532.3443, 599.0000],\n",
       "         [  0.0000, 360.2025, 180.6274, 733.2137, 434.4019],\n",
       "         ...,\n",
       "         [  0.0000,  42.1225, 423.2054, 199.4146, 519.6038],\n",
       "         [  0.0000, 702.6074,  31.8616, 819.6212, 119.1795],\n",
       "         [  0.0000, 516.5794, 267.4311, 900.0000, 599.0000]]], device='cuda:0')"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "i9X3urnBgAw_",
    "outputId": "1d1a0254-c868-4b80-9475-ea6f59d0c5d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.9014e-08, 4.2646e-09, 9.8677e-03,  ..., 1.2390e-08,\n",
       "          1.0830e-06, 2.3500e-07],\n",
       "         [1.5655e-05, 3.0358e-03, 1.3245e-01,  ..., 4.6102e-02,\n",
       "          3.2915e-01, 2.4627e-08],\n",
       "         [2.6691e-06, 5.5260e-05, 2.3951e-06,  ..., 3.9925e-01,\n",
       "          1.1437e-03, 6.6422e-04],\n",
       "         ...,\n",
       "         [5.3263e-02, 2.6517e-02, 2.4498e-02,  ..., 2.8588e-01,\n",
       "          7.6254e-02, 2.6756e-02],\n",
       "         [5.9077e-02, 1.4534e-04, 1.0010e-04,  ..., 6.0706e-01,\n",
       "          1.2002e-05, 9.5198e-02],\n",
       "         [5.2873e-05, 2.4816e-04, 2.5281e-02,  ..., 2.2312e-04,\n",
       "          4.6746e-07, 1.7650e-02]]], device='cuda:0', grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "id": "a6rtGWUwgCw8",
    "outputId": "1aebd065-5643-4e4e-b83a-60c3cf0b2ed2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0439,  0.2321,  0.0086,  0.1159],\n",
       "         [-0.1822,  0.5078,  0.4810,  0.0536],\n",
       "         [-0.7362,  0.0043,  0.2327, -0.4303],\n",
       "         ...,\n",
       "         [ 0.0317,  0.0226,  0.1402,  0.0134],\n",
       "         [ 0.1497, -0.1706,  0.1208, -0.1371],\n",
       "         [ 0.0566, -0.4836,  0.4191, -0.0208]]], device='cuda:0',\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "IbFD8baBgEak",
    "outputId": "e0ccb018-4a64-4658-8f10-2a50bff449f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6701, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpn_loss_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "ivTp_zeVgF4d",
    "outputId": "9b43e64e-1473-4214-fd76-cda9e5393085"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0299, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rpn_loss_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "9OHsztCZgHqG",
    "outputId": "de53c415-c2a8-40be-f397-c9c9f12a057a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.0552, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 69,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RCNN_loss_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "cMS15Mc9gJSR",
    "outputId": "6d506e13-6c0f-4507-8338-8ea09a2c979b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2323, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RCNN_loss_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "b58mHl3XgLNF",
    "outputId": "352733e1-923d-4df7-85ee-0fd5980a31ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rois_label"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "fasterrccvgg16.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
